{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mnist dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))])  \n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "batch_size = 64\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, drop_last=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return F.one_hot(y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = to_categorical(y, 10)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 7, 3, 2, 6, 8, 7, 0, 9])\n",
      "tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(0, 10, (10,))\n",
    "print(y)\n",
    "y = to_categorical(y, 10)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infoGAN Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, n_classes, code_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.code_dim = code_dim\n",
    "        input_dim = latent_dim + n_classes + code_dim\n",
    "        img_size = 28\n",
    "        self.init_size = img_size // 4\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128 * self.init_size ** 2)\n",
    "        )\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 1, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels, code):\n",
    "        gen_input = torch.cat((noise, labels, code), -1)\n",
    "        img = self.l1(gen_input)\n",
    "        img = img.view(img.size(0), 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "n_classes = 10\n",
    "code_dim = 5\n",
    "generator = Generator(latent_dim, n_classes, code_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100]) torch.Size([64, 10]) torch.Size([64, 5])\n",
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "noise = torch.randn(64, latent_dim).to(device)\n",
    "labels = torch.randint(0, 10, (64,)).to(device)\n",
    "labels = to_categorical(labels, n_classes).to(device)\n",
    "code = torch.randn(64, code_dim).to(device)\n",
    "print(noise.shape, labels.shape, code.shape)\n",
    "gen_out = generator(noise, labels, code)\n",
    "print(gen_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_classes, code_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.code_dim = code_dim\n",
    "        img_size = 28\n",
    "        ds_size = img_size // 4\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(16, 32, 3, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        out = 1024\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(out, 1))\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(out, n_classes))\n",
    "        self.code_layer = nn.Sequential(nn.Linear(out, code_dim))\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "        code = self.code_layer(out)\n",
    "        return validity, label, code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1]) torch.Size([64, 10]) torch.Size([64, 5])\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator(n_classes, code_dim).to(device)\n",
    "disc_out = discriminator(gen_out)\n",
    "print(disc_out[0].shape, disc_out[1].shape, disc_out[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss functions\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "continuous_loss = torch.nn.MSELoss()\n",
    "\n",
    "#optimizer\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999  \n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50 Batch 0/937                   D_loss: 9.0031 G_loss: 4.5692\n",
      "Epoch 0/50 Batch 100/937                   D_loss: 3.6459 G_loss: 2.4094\n",
      "Epoch 0/50 Batch 200/937                   D_loss: 2.3393 G_loss: 0.9006\n",
      "Epoch 0/50 Batch 300/937                   D_loss: 2.3569 G_loss: 0.7769\n",
      "Epoch 0/50 Batch 400/937                   D_loss: 2.1410 G_loss: 0.8125\n",
      "Epoch 0/50 Batch 500/937                   D_loss: 2.1598 G_loss: 0.7808\n",
      "Epoch 0/50 Batch 600/937                   D_loss: 1.8760 G_loss: 0.6769\n",
      "Epoch 0/50 Batch 700/937                   D_loss: 1.6585 G_loss: 0.6814\n",
      "Epoch 0/50 Batch 800/937                   D_loss: 1.9087 G_loss: 0.8367\n",
      "Epoch 0/50 Batch 900/937                   D_loss: 1.9352 G_loss: 0.7284\n",
      "Epoch 1/50 Batch 0/937                   D_loss: 1.9893 G_loss: 0.6374\n",
      "Epoch 1/50 Batch 100/937                   D_loss: 1.8601 G_loss: 0.7364\n",
      "Epoch 1/50 Batch 200/937                   D_loss: 1.7356 G_loss: 0.8164\n",
      "Epoch 1/50 Batch 300/937                   D_loss: 2.0802 G_loss: 0.3313\n",
      "Epoch 1/50 Batch 400/937                   D_loss: 1.5989 G_loss: 0.7590\n",
      "Epoch 1/50 Batch 500/937                   D_loss: 1.7950 G_loss: 0.8875\n",
      "Epoch 1/50 Batch 600/937                   D_loss: 1.6577 G_loss: 0.7961\n",
      "Epoch 1/50 Batch 700/937                   D_loss: 1.8771 G_loss: 0.7069\n",
      "Epoch 1/50 Batch 800/937                   D_loss: 1.6954 G_loss: 0.7317\n",
      "Epoch 1/50 Batch 900/937                   D_loss: 1.8059 G_loss: 0.7074\n",
      "Epoch 2/50 Batch 0/937                   D_loss: 1.7291 G_loss: 0.6939\n",
      "Epoch 2/50 Batch 100/937                   D_loss: 1.6777 G_loss: 0.5043\n",
      "Epoch 2/50 Batch 200/937                   D_loss: 1.7240 G_loss: 0.5004\n",
      "Epoch 2/50 Batch 300/937                   D_loss: 1.5758 G_loss: 0.6125\n",
      "Epoch 2/50 Batch 400/937                   D_loss: 1.8965 G_loss: 0.6807\n",
      "Epoch 2/50 Batch 500/937                   D_loss: 1.8413 G_loss: 0.7468\n",
      "Epoch 2/50 Batch 600/937                   D_loss: 1.6892 G_loss: 0.7784\n",
      "Epoch 2/50 Batch 700/937                   D_loss: 2.0883 G_loss: 0.6757\n",
      "Epoch 2/50 Batch 800/937                   D_loss: 2.1004 G_loss: 0.4833\n",
      "Epoch 2/50 Batch 900/937                   D_loss: 1.7914 G_loss: 0.6194\n",
      "Epoch 3/50 Batch 0/937                   D_loss: 1.8019 G_loss: 0.7685\n",
      "Epoch 3/50 Batch 100/937                   D_loss: 1.8066 G_loss: 0.7279\n",
      "Epoch 3/50 Batch 200/937                   D_loss: 1.7503 G_loss: 0.7233\n",
      "Epoch 3/50 Batch 300/937                   D_loss: 1.9281 G_loss: 0.7419\n",
      "Epoch 3/50 Batch 400/937                   D_loss: 1.8124 G_loss: 0.6244\n",
      "Epoch 3/50 Batch 500/937                   D_loss: 1.5056 G_loss: 0.5537\n",
      "Epoch 3/50 Batch 600/937                   D_loss: 1.8543 G_loss: 0.6097\n",
      "Epoch 3/50 Batch 700/937                   D_loss: 1.8661 G_loss: 0.6877\n",
      "Epoch 3/50 Batch 800/937                   D_loss: 1.5917 G_loss: 0.4819\n",
      "Epoch 3/50 Batch 900/937                   D_loss: 1.8619 G_loss: 0.5523\n",
      "Epoch 4/50 Batch 0/937                   D_loss: 1.6054 G_loss: 0.5844\n",
      "Epoch 4/50 Batch 100/937                   D_loss: 1.7440 G_loss: 0.6570\n",
      "Epoch 4/50 Batch 200/937                   D_loss: 2.0016 G_loss: 0.6900\n",
      "Epoch 4/50 Batch 300/937                   D_loss: 1.8258 G_loss: 0.5060\n",
      "Epoch 4/50 Batch 400/937                   D_loss: 1.7557 G_loss: 0.7004\n",
      "Epoch 4/50 Batch 500/937                   D_loss: 1.6183 G_loss: 0.5538\n",
      "Epoch 4/50 Batch 600/937                   D_loss: 1.6319 G_loss: 0.5682\n",
      "Epoch 4/50 Batch 700/937                   D_loss: 1.9303 G_loss: 0.5226\n",
      "Epoch 4/50 Batch 800/937                   D_loss: 1.6486 G_loss: 0.6020\n",
      "Epoch 4/50 Batch 900/937                   D_loss: 1.7022 G_loss: 0.6428\n",
      "Epoch 5/50 Batch 0/937                   D_loss: 1.8068 G_loss: 0.6885\n",
      "Epoch 5/50 Batch 100/937                   D_loss: 1.8507 G_loss: 0.5271\n",
      "Epoch 5/50 Batch 200/937                   D_loss: 1.8245 G_loss: 0.6944\n",
      "Epoch 5/50 Batch 300/937                   D_loss: 1.6497 G_loss: 0.6060\n",
      "Epoch 5/50 Batch 400/937                   D_loss: 1.6277 G_loss: 0.5835\n",
      "Epoch 5/50 Batch 500/937                   D_loss: 2.0084 G_loss: 0.7115\n",
      "Epoch 5/50 Batch 600/937                   D_loss: 2.0949 G_loss: 0.5942\n",
      "Epoch 5/50 Batch 700/937                   D_loss: 1.7354 G_loss: 0.5606\n",
      "Epoch 5/50 Batch 800/937                   D_loss: 1.7933 G_loss: 0.5653\n",
      "Epoch 5/50 Batch 900/937                   D_loss: 1.8778 G_loss: 0.6431\n",
      "Epoch 6/50 Batch 0/937                   D_loss: 1.9029 G_loss: 0.5409\n",
      "Epoch 6/50 Batch 100/937                   D_loss: 1.7475 G_loss: 0.5242\n",
      "Epoch 6/50 Batch 200/937                   D_loss: 1.7316 G_loss: 0.6523\n",
      "Epoch 6/50 Batch 300/937                   D_loss: 1.5964 G_loss: 0.7098\n",
      "Epoch 6/50 Batch 400/937                   D_loss: 1.6533 G_loss: 0.4343\n",
      "Epoch 6/50 Batch 500/937                   D_loss: 1.6895 G_loss: 0.6897\n",
      "Epoch 6/50 Batch 600/937                   D_loss: 1.9623 G_loss: 0.5522\n",
      "Epoch 6/50 Batch 700/937                   D_loss: 1.7800 G_loss: 0.6402\n",
      "Epoch 6/50 Batch 800/937                   D_loss: 1.7800 G_loss: 0.6505\n",
      "Epoch 6/50 Batch 900/937                   D_loss: 1.7727 G_loss: 0.5075\n",
      "Epoch 7/50 Batch 0/937                   D_loss: 1.7960 G_loss: 0.6280\n",
      "Epoch 7/50 Batch 100/937                   D_loss: 1.6447 G_loss: 0.6321\n",
      "Epoch 7/50 Batch 200/937                   D_loss: 1.6395 G_loss: 0.6243\n",
      "Epoch 7/50 Batch 300/937                   D_loss: 1.9639 G_loss: 0.5534\n",
      "Epoch 7/50 Batch 400/937                   D_loss: 1.7389 G_loss: 0.6895\n",
      "Epoch 7/50 Batch 500/937                   D_loss: 1.7367 G_loss: 0.4834\n",
      "Epoch 7/50 Batch 600/937                   D_loss: 1.9168 G_loss: 0.4123\n",
      "Epoch 7/50 Batch 700/937                   D_loss: 1.7293 G_loss: 0.5031\n",
      "Epoch 7/50 Batch 800/937                   D_loss: 1.7463 G_loss: 0.5498\n",
      "Epoch 7/50 Batch 900/937                   D_loss: 1.8577 G_loss: 0.5446\n",
      "Epoch 8/50 Batch 0/937                   D_loss: 1.7952 G_loss: 0.5423\n",
      "Epoch 8/50 Batch 100/937                   D_loss: 1.8372 G_loss: 0.5309\n",
      "Epoch 8/50 Batch 200/937                   D_loss: 1.8344 G_loss: 0.6351\n",
      "Epoch 8/50 Batch 300/937                   D_loss: 1.5664 G_loss: 0.5369\n",
      "Epoch 8/50 Batch 400/937                   D_loss: 1.6140 G_loss: 0.5267\n",
      "Epoch 8/50 Batch 500/937                   D_loss: 1.8350 G_loss: 0.4994\n",
      "Epoch 8/50 Batch 600/937                   D_loss: 1.7591 G_loss: 0.5891\n",
      "Epoch 8/50 Batch 700/937                   D_loss: 1.6578 G_loss: 0.5234\n",
      "Epoch 8/50 Batch 800/937                   D_loss: 1.6661 G_loss: 0.5480\n",
      "Epoch 8/50 Batch 900/937                   D_loss: 1.8978 G_loss: 0.5307\n",
      "Epoch 9/50 Batch 0/937                   D_loss: 1.7105 G_loss: 0.5604\n",
      "Epoch 9/50 Batch 100/937                   D_loss: 1.6483 G_loss: 0.5721\n",
      "Epoch 9/50 Batch 200/937                   D_loss: 1.6220 G_loss: 0.6821\n",
      "Epoch 9/50 Batch 300/937                   D_loss: 1.7104 G_loss: 0.5034\n",
      "Epoch 9/50 Batch 400/937                   D_loss: 1.7947 G_loss: 0.5893\n",
      "Epoch 9/50 Batch 500/937                   D_loss: 1.7156 G_loss: 0.5145\n",
      "Epoch 9/50 Batch 600/937                   D_loss: 1.8400 G_loss: 0.3866\n",
      "Epoch 9/50 Batch 700/937                   D_loss: 1.5737 G_loss: 0.4668\n",
      "Epoch 9/50 Batch 800/937                   D_loss: 1.7935 G_loss: 0.6264\n",
      "Epoch 9/50 Batch 900/937                   D_loss: 1.7744 G_loss: 0.5058\n",
      "Epoch 10/50 Batch 0/937                   D_loss: 1.5250 G_loss: 0.7242\n",
      "Epoch 10/50 Batch 100/937                   D_loss: 1.7425 G_loss: 0.5302\n",
      "Epoch 10/50 Batch 200/937                   D_loss: 1.9080 G_loss: 0.5918\n",
      "Epoch 10/50 Batch 300/937                   D_loss: 1.8395 G_loss: 0.5788\n",
      "Epoch 10/50 Batch 400/937                   D_loss: 1.7561 G_loss: 0.5795\n",
      "Epoch 10/50 Batch 500/937                   D_loss: 1.7660 G_loss: 0.4983\n",
      "Epoch 10/50 Batch 600/937                   D_loss: 1.7194 G_loss: 0.5138\n",
      "Epoch 10/50 Batch 700/937                   D_loss: 1.5759 G_loss: 0.6095\n",
      "Epoch 10/50 Batch 800/937                   D_loss: 1.6169 G_loss: 0.6928\n",
      "Epoch 10/50 Batch 900/937                   D_loss: 1.6351 G_loss: 0.4161\n",
      "Epoch 11/50 Batch 0/937                   D_loss: 1.6933 G_loss: 0.5103\n",
      "Epoch 11/50 Batch 100/937                   D_loss: 1.8603 G_loss: 0.5231\n",
      "Epoch 11/50 Batch 200/937                   D_loss: 1.9837 G_loss: 0.4051\n",
      "Epoch 11/50 Batch 300/937                   D_loss: 1.5065 G_loss: 0.6108\n",
      "Epoch 11/50 Batch 400/937                   D_loss: 1.5990 G_loss: 0.5583\n",
      "Epoch 11/50 Batch 500/937                   D_loss: 1.5480 G_loss: 0.4946\n",
      "Epoch 11/50 Batch 600/937                   D_loss: 1.5752 G_loss: 0.4801\n",
      "Epoch 11/50 Batch 700/937                   D_loss: 1.5697 G_loss: 0.5193\n",
      "Epoch 11/50 Batch 800/937                   D_loss: 1.8065 G_loss: 0.5511\n",
      "Epoch 11/50 Batch 900/937                   D_loss: 1.6034 G_loss: 0.3883\n",
      "Epoch 12/50 Batch 0/937                   D_loss: 1.7916 G_loss: 0.4038\n",
      "Epoch 12/50 Batch 100/937                   D_loss: 1.6887 G_loss: 0.4506\n",
      "Epoch 12/50 Batch 200/937                   D_loss: 1.5320 G_loss: 0.6036\n",
      "Epoch 12/50 Batch 300/937                   D_loss: 1.7410 G_loss: 0.5842\n",
      "Epoch 12/50 Batch 400/937                   D_loss: 1.7212 G_loss: 0.5765\n",
      "Epoch 12/50 Batch 500/937                   D_loss: 1.6998 G_loss: 0.6109\n",
      "Epoch 12/50 Batch 600/937                   D_loss: 1.8006 G_loss: 0.4520\n",
      "Epoch 12/50 Batch 700/937                   D_loss: 1.7669 G_loss: 0.4618\n",
      "Epoch 12/50 Batch 800/937                   D_loss: 1.8183 G_loss: 0.7129\n",
      "Epoch 12/50 Batch 900/937                   D_loss: 1.4393 G_loss: 0.5440\n",
      "Epoch 13/50 Batch 0/937                   D_loss: 1.7448 G_loss: 0.5032\n",
      "Epoch 13/50 Batch 100/937                   D_loss: 1.7066 G_loss: 0.6818\n",
      "Epoch 13/50 Batch 200/937                   D_loss: 1.7067 G_loss: 0.5179\n",
      "Epoch 13/50 Batch 300/937                   D_loss: 1.6511 G_loss: 0.7376\n",
      "Epoch 13/50 Batch 400/937                   D_loss: 1.8819 G_loss: 0.5721\n",
      "Epoch 13/50 Batch 500/937                   D_loss: 1.9311 G_loss: 0.4211\n",
      "Epoch 13/50 Batch 600/937                   D_loss: 1.8260 G_loss: 0.4452\n",
      "Epoch 13/50 Batch 700/937                   D_loss: 1.8351 G_loss: 0.6280\n",
      "Epoch 13/50 Batch 800/937                   D_loss: 1.6618 G_loss: 0.4016\n",
      "Epoch 13/50 Batch 900/937                   D_loss: 1.6908 G_loss: 0.4433\n",
      "Epoch 14/50 Batch 0/937                   D_loss: 1.6099 G_loss: 0.5322\n",
      "Epoch 14/50 Batch 100/937                   D_loss: 1.6511 G_loss: 0.5162\n",
      "Epoch 14/50 Batch 200/937                   D_loss: 1.5819 G_loss: 0.5364\n",
      "Epoch 14/50 Batch 300/937                   D_loss: 1.7658 G_loss: 0.4537\n",
      "Epoch 14/50 Batch 400/937                   D_loss: 1.6577 G_loss: 0.4251\n",
      "Epoch 14/50 Batch 500/937                   D_loss: 1.9244 G_loss: 0.3867\n",
      "Epoch 14/50 Batch 600/937                   D_loss: 1.7101 G_loss: 0.4964\n",
      "Epoch 14/50 Batch 700/937                   D_loss: 1.5897 G_loss: 0.5860\n",
      "Epoch 14/50 Batch 800/937                   D_loss: 1.5516 G_loss: 0.4664\n",
      "Epoch 14/50 Batch 900/937                   D_loss: 1.6039 G_loss: 0.5169\n",
      "Epoch 15/50 Batch 0/937                   D_loss: 1.7833 G_loss: 0.4867\n",
      "Epoch 15/50 Batch 100/937                   D_loss: 1.6908 G_loss: 0.4306\n",
      "Epoch 15/50 Batch 200/937                   D_loss: 1.5350 G_loss: 0.5588\n",
      "Epoch 15/50 Batch 300/937                   D_loss: 1.6651 G_loss: 0.3724\n",
      "Epoch 15/50 Batch 400/937                   D_loss: 1.4928 G_loss: 0.4904\n",
      "Epoch 15/50 Batch 500/937                   D_loss: 1.6345 G_loss: 0.5008\n",
      "Epoch 15/50 Batch 600/937                   D_loss: 1.6252 G_loss: 0.4734\n",
      "Epoch 15/50 Batch 700/937                   D_loss: 1.5920 G_loss: 0.6048\n",
      "Epoch 15/50 Batch 800/937                   D_loss: 1.5133 G_loss: 0.6954\n",
      "Epoch 15/50 Batch 900/937                   D_loss: 1.9143 G_loss: 0.3744\n",
      "Epoch 16/50 Batch 0/937                   D_loss: 1.7488 G_loss: 0.6753\n",
      "Epoch 16/50 Batch 100/937                   D_loss: 1.7130 G_loss: 0.4743\n",
      "Epoch 16/50 Batch 200/937                   D_loss: 1.7944 G_loss: 0.5028\n",
      "Epoch 16/50 Batch 300/937                   D_loss: 1.7338 G_loss: 0.3843\n",
      "Epoch 16/50 Batch 400/937                   D_loss: 1.7301 G_loss: 0.4673\n",
      "Epoch 16/50 Batch 500/937                   D_loss: 1.6485 G_loss: 0.6415\n",
      "Epoch 16/50 Batch 600/937                   D_loss: 1.7833 G_loss: 0.5935\n",
      "Epoch 16/50 Batch 700/937                   D_loss: 1.7355 G_loss: 0.4389\n",
      "Epoch 16/50 Batch 800/937                   D_loss: 1.6851 G_loss: 0.6735\n",
      "Epoch 16/50 Batch 900/937                   D_loss: 1.5354 G_loss: 0.5468\n",
      "Epoch 17/50 Batch 0/937                   D_loss: 1.6037 G_loss: 0.6148\n",
      "Epoch 17/50 Batch 100/937                   D_loss: 1.8563 G_loss: 0.2980\n",
      "Epoch 17/50 Batch 200/937                   D_loss: 1.7701 G_loss: 0.5281\n",
      "Epoch 17/50 Batch 300/937                   D_loss: 1.5493 G_loss: 0.4240\n",
      "Epoch 17/50 Batch 400/937                   D_loss: 1.4706 G_loss: 0.5589\n",
      "Epoch 17/50 Batch 500/937                   D_loss: 1.6919 G_loss: 0.5234\n",
      "Epoch 17/50 Batch 600/937                   D_loss: 1.8279 G_loss: 0.3997\n",
      "Epoch 17/50 Batch 700/937                   D_loss: 1.6556 G_loss: 0.4329\n",
      "Epoch 17/50 Batch 800/937                   D_loss: 1.7238 G_loss: 0.2750\n",
      "Epoch 17/50 Batch 900/937                   D_loss: 1.7782 G_loss: 0.5035\n",
      "Epoch 18/50 Batch 0/937                   D_loss: 1.5953 G_loss: 0.5089\n",
      "Epoch 18/50 Batch 100/937                   D_loss: 1.4722 G_loss: 0.4941\n",
      "Epoch 18/50 Batch 200/937                   D_loss: 1.9028 G_loss: 1.0460\n",
      "Epoch 18/50 Batch 300/937                   D_loss: 1.7219 G_loss: 0.4948\n",
      "Epoch 18/50 Batch 400/937                   D_loss: 1.6240 G_loss: 0.4334\n",
      "Epoch 18/50 Batch 500/937                   D_loss: 1.5538 G_loss: 0.4255\n",
      "Epoch 18/50 Batch 600/937                   D_loss: 1.6472 G_loss: 0.4315\n",
      "Epoch 18/50 Batch 700/937                   D_loss: 1.6971 G_loss: 0.4649\n",
      "Epoch 18/50 Batch 800/937                   D_loss: 1.5564 G_loss: 0.4256\n",
      "Epoch 18/50 Batch 900/937                   D_loss: 1.7011 G_loss: 0.4557\n",
      "Epoch 19/50 Batch 0/937                   D_loss: 1.5532 G_loss: 0.6689\n",
      "Epoch 19/50 Batch 100/937                   D_loss: 1.7181 G_loss: 0.4919\n",
      "Epoch 19/50 Batch 200/937                   D_loss: 1.5021 G_loss: 0.4311\n",
      "Epoch 19/50 Batch 300/937                   D_loss: 1.7357 G_loss: 0.4165\n",
      "Epoch 19/50 Batch 400/937                   D_loss: 1.4828 G_loss: 0.5049\n",
      "Epoch 19/50 Batch 500/937                   D_loss: 1.5920 G_loss: 0.4612\n",
      "Epoch 19/50 Batch 600/937                   D_loss: 1.8002 G_loss: 0.4840\n",
      "Epoch 19/50 Batch 700/937                   D_loss: 1.7878 G_loss: 0.3739\n",
      "Epoch 19/50 Batch 800/937                   D_loss: 1.6218 G_loss: 0.5179\n",
      "Epoch 19/50 Batch 900/937                   D_loss: 1.6345 G_loss: 0.4881\n",
      "Epoch 20/50 Batch 0/937                   D_loss: 1.8516 G_loss: 0.5420\n",
      "Epoch 20/50 Batch 100/937                   D_loss: 1.6456 G_loss: 0.4999\n",
      "Epoch 20/50 Batch 200/937                   D_loss: 1.6986 G_loss: 0.3446\n",
      "Epoch 20/50 Batch 300/937                   D_loss: 1.7649 G_loss: 0.6414\n",
      "Epoch 20/50 Batch 400/937                   D_loss: 1.7376 G_loss: 0.4788\n",
      "Epoch 20/50 Batch 500/937                   D_loss: 1.5397 G_loss: 0.3972\n",
      "Epoch 20/50 Batch 600/937                   D_loss: 1.5784 G_loss: 0.5056\n",
      "Epoch 20/50 Batch 700/937                   D_loss: 1.9210 G_loss: 0.4683\n",
      "Epoch 20/50 Batch 800/937                   D_loss: 1.4432 G_loss: 0.4738\n",
      "Epoch 20/50 Batch 900/937                   D_loss: 1.5770 G_loss: 0.3165\n",
      "Epoch 21/50 Batch 0/937                   D_loss: 1.7111 G_loss: 0.4982\n",
      "Epoch 21/50 Batch 100/937                   D_loss: 1.5666 G_loss: 0.6059\n",
      "Epoch 21/50 Batch 200/937                   D_loss: 1.6706 G_loss: 0.5107\n",
      "Epoch 21/50 Batch 300/937                   D_loss: 1.6033 G_loss: 0.4599\n",
      "Epoch 21/50 Batch 400/937                   D_loss: 1.6933 G_loss: 0.4811\n",
      "Epoch 21/50 Batch 500/937                   D_loss: 1.8591 G_loss: 0.4401\n",
      "Epoch 21/50 Batch 600/937                   D_loss: 1.5122 G_loss: 0.4726\n",
      "Epoch 21/50 Batch 700/937                   D_loss: 1.5744 G_loss: 0.4209\n",
      "Epoch 21/50 Batch 800/937                   D_loss: 1.7810 G_loss: 0.5187\n",
      "Epoch 21/50 Batch 900/937                   D_loss: 1.6212 G_loss: 0.4223\n",
      "Epoch 22/50 Batch 0/937                   D_loss: 1.5288 G_loss: 0.5042\n",
      "Epoch 22/50 Batch 100/937                   D_loss: 1.7684 G_loss: 0.2218\n",
      "Epoch 22/50 Batch 200/937                   D_loss: 1.6264 G_loss: 0.4257\n",
      "Epoch 22/50 Batch 300/937                   D_loss: 1.6778 G_loss: 0.2629\n",
      "Epoch 22/50 Batch 400/937                   D_loss: 1.5255 G_loss: 0.6677\n",
      "Epoch 22/50 Batch 500/937                   D_loss: 1.8072 G_loss: 0.2238\n",
      "Epoch 22/50 Batch 600/937                   D_loss: 1.6239 G_loss: 0.4171\n",
      "Epoch 22/50 Batch 700/937                   D_loss: 1.7523 G_loss: 0.4188\n",
      "Epoch 22/50 Batch 800/937                   D_loss: 1.6690 G_loss: 0.4983\n",
      "Epoch 22/50 Batch 900/937                   D_loss: 1.5047 G_loss: 0.4679\n",
      "Epoch 23/50 Batch 0/937                   D_loss: 1.5768 G_loss: 0.2943\n",
      "Epoch 23/50 Batch 100/937                   D_loss: 1.5998 G_loss: 0.3849\n",
      "Epoch 23/50 Batch 200/937                   D_loss: 1.5573 G_loss: 0.4006\n",
      "Epoch 23/50 Batch 300/937                   D_loss: 1.6618 G_loss: 0.5895\n",
      "Epoch 23/50 Batch 400/937                   D_loss: 1.6798 G_loss: 0.5761\n",
      "Epoch 23/50 Batch 500/937                   D_loss: 1.5395 G_loss: 0.4012\n",
      "Epoch 23/50 Batch 600/937                   D_loss: 1.6611 G_loss: 0.5144\n",
      "Epoch 23/50 Batch 700/937                   D_loss: 1.5925 G_loss: 0.4351\n",
      "Epoch 23/50 Batch 800/937                   D_loss: 1.6375 G_loss: 0.4792\n",
      "Epoch 23/50 Batch 900/937                   D_loss: 1.9475 G_loss: 0.4518\n",
      "Epoch 24/50 Batch 0/937                   D_loss: 1.5740 G_loss: 0.3427\n",
      "Epoch 24/50 Batch 100/937                   D_loss: 1.5788 G_loss: 0.4855\n",
      "Epoch 24/50 Batch 200/937                   D_loss: 1.8526 G_loss: 0.6100\n",
      "Epoch 24/50 Batch 300/937                   D_loss: 1.6606 G_loss: 0.4568\n",
      "Epoch 24/50 Batch 400/937                   D_loss: 1.5322 G_loss: 0.3694\n",
      "Epoch 24/50 Batch 500/937                   D_loss: 1.6860 G_loss: 0.4917\n",
      "Epoch 24/50 Batch 600/937                   D_loss: 1.9286 G_loss: 0.4699\n",
      "Epoch 24/50 Batch 700/937                   D_loss: 1.5628 G_loss: 0.4899\n",
      "Epoch 24/50 Batch 800/937                   D_loss: 1.5898 G_loss: 0.3858\n",
      "Epoch 24/50 Batch 900/937                   D_loss: 1.6549 G_loss: 0.4490\n",
      "Epoch 25/50 Batch 0/937                   D_loss: 1.4995 G_loss: 0.5024\n",
      "Epoch 25/50 Batch 100/937                   D_loss: 1.7283 G_loss: 0.4461\n",
      "Epoch 25/50 Batch 200/937                   D_loss: 1.7469 G_loss: 0.4131\n",
      "Epoch 25/50 Batch 300/937                   D_loss: 1.6086 G_loss: 0.5650\n",
      "Epoch 25/50 Batch 400/937                   D_loss: 1.6902 G_loss: 0.3971\n",
      "Epoch 25/50 Batch 500/937                   D_loss: 1.7334 G_loss: 0.5690\n",
      "Epoch 25/50 Batch 600/937                   D_loss: 1.6318 G_loss: 0.3627\n",
      "Epoch 25/50 Batch 700/937                   D_loss: 1.7897 G_loss: 0.3758\n",
      "Epoch 25/50 Batch 800/937                   D_loss: 1.7603 G_loss: 0.4027\n",
      "Epoch 25/50 Batch 900/937                   D_loss: 1.5915 G_loss: 0.4494\n",
      "Epoch 26/50 Batch 0/937                   D_loss: 1.4625 G_loss: 0.3564\n",
      "Epoch 26/50 Batch 100/937                   D_loss: 1.7629 G_loss: 0.3574\n",
      "Epoch 26/50 Batch 200/937                   D_loss: 1.5443 G_loss: 0.4132\n",
      "Epoch 26/50 Batch 300/937                   D_loss: 1.6086 G_loss: 0.4212\n",
      "Epoch 26/50 Batch 400/937                   D_loss: 1.6414 G_loss: 0.4641\n",
      "Epoch 26/50 Batch 500/937                   D_loss: 1.6260 G_loss: 0.3781\n",
      "Epoch 26/50 Batch 600/937                   D_loss: 1.6197 G_loss: 0.4215\n",
      "Epoch 26/50 Batch 700/937                   D_loss: 1.6745 G_loss: 0.4582\n",
      "Epoch 26/50 Batch 800/937                   D_loss: 1.6159 G_loss: 0.4829\n",
      "Epoch 26/50 Batch 900/937                   D_loss: 1.6962 G_loss: 0.4163\n",
      "Epoch 27/50 Batch 0/937                   D_loss: 1.7584 G_loss: 0.4373\n",
      "Epoch 27/50 Batch 100/937                   D_loss: 1.3828 G_loss: 0.4870\n",
      "Epoch 27/50 Batch 200/937                   D_loss: 1.6196 G_loss: 0.4018\n",
      "Epoch 27/50 Batch 300/937                   D_loss: 1.6206 G_loss: 0.4457\n",
      "Epoch 27/50 Batch 400/937                   D_loss: 1.8085 G_loss: 0.4721\n",
      "Epoch 27/50 Batch 500/937                   D_loss: 1.6593 G_loss: 0.2926\n",
      "Epoch 27/50 Batch 600/937                   D_loss: 1.7707 G_loss: 0.4395\n",
      "Epoch 27/50 Batch 700/937                   D_loss: 1.5458 G_loss: 0.4232\n",
      "Epoch 27/50 Batch 800/937                   D_loss: 1.5994 G_loss: 0.4435\n",
      "Epoch 27/50 Batch 900/937                   D_loss: 1.8200 G_loss: 0.5589\n",
      "Epoch 28/50 Batch 0/937                   D_loss: 1.5641 G_loss: 0.3672\n",
      "Epoch 28/50 Batch 100/937                   D_loss: 1.6081 G_loss: 0.4730\n",
      "Epoch 28/50 Batch 200/937                   D_loss: 1.6704 G_loss: 0.3904\n",
      "Epoch 28/50 Batch 300/937                   D_loss: 1.3597 G_loss: 0.4156\n",
      "Epoch 28/50 Batch 400/937                   D_loss: 1.5743 G_loss: 0.3778\n",
      "Epoch 28/50 Batch 500/937                   D_loss: 1.5381 G_loss: 0.3720\n",
      "Epoch 28/50 Batch 600/937                   D_loss: 1.9270 G_loss: 0.2872\n",
      "Epoch 28/50 Batch 700/937                   D_loss: 1.5688 G_loss: 0.4791\n",
      "Epoch 28/50 Batch 800/937                   D_loss: 1.7641 G_loss: 0.3710\n",
      "Epoch 28/50 Batch 900/937                   D_loss: 1.6862 G_loss: 0.3815\n",
      "Epoch 29/50 Batch 0/937                   D_loss: 1.6982 G_loss: 0.3042\n",
      "Epoch 29/50 Batch 100/937                   D_loss: 1.5341 G_loss: 0.5073\n",
      "Epoch 29/50 Batch 200/937                   D_loss: 1.4526 G_loss: 0.3244\n",
      "Epoch 29/50 Batch 300/937                   D_loss: 1.5606 G_loss: 0.4806\n",
      "Epoch 29/50 Batch 400/937                   D_loss: 1.5571 G_loss: 0.4800\n",
      "Epoch 29/50 Batch 500/937                   D_loss: 1.5818 G_loss: 0.3303\n",
      "Epoch 29/50 Batch 600/937                   D_loss: 1.6211 G_loss: 0.3726\n",
      "Epoch 29/50 Batch 700/937                   D_loss: 1.5414 G_loss: 0.4970\n",
      "Epoch 29/50 Batch 800/937                   D_loss: 1.6368 G_loss: 0.4155\n",
      "Epoch 29/50 Batch 900/937                   D_loss: 1.5744 G_loss: 0.4785\n",
      "Epoch 30/50 Batch 0/937                   D_loss: 1.5861 G_loss: 0.4767\n",
      "Epoch 30/50 Batch 100/937                   D_loss: 1.7063 G_loss: 0.4816\n",
      "Epoch 30/50 Batch 200/937                   D_loss: 1.5710 G_loss: 0.4833\n",
      "Epoch 30/50 Batch 300/937                   D_loss: 1.5166 G_loss: 0.5554\n",
      "Epoch 30/50 Batch 400/937                   D_loss: 1.5247 G_loss: 0.4499\n",
      "Epoch 30/50 Batch 500/937                   D_loss: 1.5751 G_loss: 0.4548\n",
      "Epoch 30/50 Batch 600/937                   D_loss: 1.6157 G_loss: 0.4455\n",
      "Epoch 30/50 Batch 700/937                   D_loss: 1.6224 G_loss: 0.3022\n",
      "Epoch 30/50 Batch 800/937                   D_loss: 1.5713 G_loss: 0.3049\n",
      "Epoch 30/50 Batch 900/937                   D_loss: 1.6107 G_loss: 0.5290\n",
      "Epoch 31/50 Batch 0/937                   D_loss: 1.4216 G_loss: 0.4247\n",
      "Epoch 31/50 Batch 100/937                   D_loss: 1.6659 G_loss: 0.3268\n",
      "Epoch 31/50 Batch 200/937                   D_loss: 1.5357 G_loss: 0.4736\n",
      "Epoch 31/50 Batch 300/937                   D_loss: 1.4819 G_loss: 0.5027\n",
      "Epoch 31/50 Batch 400/937                   D_loss: 1.6263 G_loss: 0.4112\n",
      "Epoch 31/50 Batch 500/937                   D_loss: 1.6084 G_loss: 0.4580\n",
      "Epoch 31/50 Batch 600/937                   D_loss: 1.6617 G_loss: 0.6479\n",
      "Epoch 31/50 Batch 700/937                   D_loss: 1.6630 G_loss: 0.3912\n",
      "Epoch 31/50 Batch 800/937                   D_loss: 1.6434 G_loss: 0.6132\n",
      "Epoch 31/50 Batch 900/937                   D_loss: 1.6106 G_loss: 0.6668\n",
      "Epoch 32/50 Batch 0/937                   D_loss: 1.4688 G_loss: 0.4923\n",
      "Epoch 32/50 Batch 100/937                   D_loss: 1.7220 G_loss: 0.5268\n",
      "Epoch 32/50 Batch 200/937                   D_loss: 1.5308 G_loss: 0.3961\n",
      "Epoch 32/50 Batch 300/937                   D_loss: 1.6847 G_loss: 0.5859\n",
      "Epoch 32/50 Batch 400/937                   D_loss: 1.7120 G_loss: 0.4963\n",
      "Epoch 32/50 Batch 500/937                   D_loss: 1.5023 G_loss: 0.6620\n",
      "Epoch 32/50 Batch 600/937                   D_loss: 1.5785 G_loss: 0.4255\n",
      "Epoch 32/50 Batch 700/937                   D_loss: 1.6837 G_loss: 0.5364\n",
      "Epoch 32/50 Batch 800/937                   D_loss: 1.5649 G_loss: 0.4171\n",
      "Epoch 32/50 Batch 900/937                   D_loss: 1.5698 G_loss: 0.3177\n",
      "Epoch 33/50 Batch 0/937                   D_loss: 1.3324 G_loss: 0.3518\n",
      "Epoch 33/50 Batch 100/937                   D_loss: 1.4949 G_loss: 0.5678\n",
      "Epoch 33/50 Batch 200/937                   D_loss: 1.8147 G_loss: 0.4100\n",
      "Epoch 33/50 Batch 300/937                   D_loss: 1.5734 G_loss: 0.3563\n",
      "Epoch 33/50 Batch 400/937                   D_loss: 1.5467 G_loss: 0.5735\n",
      "Epoch 33/50 Batch 500/937                   D_loss: 1.4774 G_loss: 0.5043\n",
      "Epoch 33/50 Batch 600/937                   D_loss: 1.8049 G_loss: 0.3686\n",
      "Epoch 33/50 Batch 700/937                   D_loss: 1.5949 G_loss: 0.5406\n",
      "Epoch 33/50 Batch 800/937                   D_loss: 1.6106 G_loss: 0.5828\n",
      "Epoch 33/50 Batch 900/937                   D_loss: 1.7152 G_loss: 0.3710\n",
      "Epoch 34/50 Batch 0/937                   D_loss: 1.5306 G_loss: 0.4527\n",
      "Epoch 34/50 Batch 100/937                   D_loss: 1.7734 G_loss: 0.4367\n",
      "Epoch 34/50 Batch 200/937                   D_loss: 1.6361 G_loss: 0.4620\n",
      "Epoch 34/50 Batch 300/937                   D_loss: 1.8079 G_loss: 0.3964\n",
      "Epoch 34/50 Batch 400/937                   D_loss: 1.5451 G_loss: 0.4829\n",
      "Epoch 34/50 Batch 500/937                   D_loss: 1.7618 G_loss: 0.2690\n",
      "Epoch 34/50 Batch 600/937                   D_loss: 1.6699 G_loss: 0.2800\n",
      "Epoch 34/50 Batch 700/937                   D_loss: 1.5384 G_loss: 0.5002\n",
      "Epoch 34/50 Batch 800/937                   D_loss: 1.5538 G_loss: 0.4059\n",
      "Epoch 34/50 Batch 900/937                   D_loss: 1.6602 G_loss: 0.5792\n",
      "Epoch 35/50 Batch 0/937                   D_loss: 1.6524 G_loss: 0.3718\n",
      "Epoch 35/50 Batch 100/937                   D_loss: 1.6121 G_loss: 0.2913\n",
      "Epoch 35/50 Batch 200/937                   D_loss: 1.6031 G_loss: 0.3155\n",
      "Epoch 35/50 Batch 300/937                   D_loss: 1.6046 G_loss: 0.3867\n",
      "Epoch 35/50 Batch 400/937                   D_loss: 1.7050 G_loss: 0.3286\n",
      "Epoch 35/50 Batch 500/937                   D_loss: 1.7336 G_loss: 0.5935\n",
      "Epoch 35/50 Batch 600/937                   D_loss: 1.4790 G_loss: 0.5122\n",
      "Epoch 35/50 Batch 700/937                   D_loss: 1.5785 G_loss: 0.3705\n",
      "Epoch 35/50 Batch 800/937                   D_loss: 1.5910 G_loss: 0.4488\n",
      "Epoch 35/50 Batch 900/937                   D_loss: 1.5164 G_loss: 0.4459\n",
      "Epoch 36/50 Batch 0/937                   D_loss: 1.6092 G_loss: 0.4081\n",
      "Epoch 36/50 Batch 100/937                   D_loss: 1.7203 G_loss: 0.3742\n",
      "Epoch 36/50 Batch 200/937                   D_loss: 1.8183 G_loss: 0.6077\n",
      "Epoch 36/50 Batch 300/937                   D_loss: 1.6112 G_loss: 0.4731\n",
      "Epoch 36/50 Batch 400/937                   D_loss: 2.1464 G_loss: 0.1116\n",
      "Epoch 36/50 Batch 500/937                   D_loss: 1.3866 G_loss: 0.5635\n",
      "Epoch 36/50 Batch 600/937                   D_loss: 1.5834 G_loss: 0.6009\n",
      "Epoch 36/50 Batch 700/937                   D_loss: 1.7364 G_loss: 0.3763\n",
      "Epoch 36/50 Batch 800/937                   D_loss: 1.7599 G_loss: 0.4877\n",
      "Epoch 36/50 Batch 900/937                   D_loss: 1.6061 G_loss: 0.5009\n",
      "Epoch 37/50 Batch 0/937                   D_loss: 1.5163 G_loss: 0.4522\n",
      "Epoch 37/50 Batch 100/937                   D_loss: 1.6790 G_loss: 0.5267\n",
      "Epoch 37/50 Batch 200/937                   D_loss: 1.6729 G_loss: 0.2975\n",
      "Epoch 37/50 Batch 300/937                   D_loss: 1.7191 G_loss: 0.4600\n",
      "Epoch 37/50 Batch 400/937                   D_loss: 1.5992 G_loss: 0.5545\n",
      "Epoch 37/50 Batch 500/937                   D_loss: 1.5035 G_loss: 0.4063\n",
      "Epoch 37/50 Batch 600/937                   D_loss: 1.5292 G_loss: 0.4567\n",
      "Epoch 37/50 Batch 700/937                   D_loss: 1.4695 G_loss: 0.3999\n",
      "Epoch 37/50 Batch 800/937                   D_loss: 1.4314 G_loss: 0.4256\n",
      "Epoch 37/50 Batch 900/937                   D_loss: 1.6105 G_loss: 0.5417\n",
      "Epoch 38/50 Batch 0/937                   D_loss: 1.4780 G_loss: 0.7276\n",
      "Epoch 38/50 Batch 100/937                   D_loss: 1.6006 G_loss: 0.4911\n",
      "Epoch 38/50 Batch 200/937                   D_loss: 1.5926 G_loss: 0.7212\n",
      "Epoch 38/50 Batch 300/937                   D_loss: 1.6155 G_loss: 0.5202\n",
      "Epoch 38/50 Batch 400/937                   D_loss: 1.6380 G_loss: 0.3625\n",
      "Epoch 38/50 Batch 500/937                   D_loss: 1.6268 G_loss: 0.4353\n",
      "Epoch 38/50 Batch 600/937                   D_loss: 1.5030 G_loss: 0.4868\n",
      "Epoch 38/50 Batch 700/937                   D_loss: 1.8219 G_loss: 0.4692\n",
      "Epoch 38/50 Batch 800/937                   D_loss: 1.6167 G_loss: 0.3519\n",
      "Epoch 38/50 Batch 900/937                   D_loss: 1.6099 G_loss: 0.4194\n",
      "Epoch 39/50 Batch 0/937                   D_loss: 1.8299 G_loss: 0.2745\n",
      "Epoch 39/50 Batch 100/937                   D_loss: 1.6528 G_loss: 0.4846\n",
      "Epoch 39/50 Batch 200/937                   D_loss: 1.6199 G_loss: 0.4186\n",
      "Epoch 39/50 Batch 300/937                   D_loss: 1.5736 G_loss: 0.2996\n",
      "Epoch 39/50 Batch 400/937                   D_loss: 1.6490 G_loss: 0.4105\n",
      "Epoch 39/50 Batch 500/937                   D_loss: 1.6327 G_loss: 0.3744\n",
      "Epoch 39/50 Batch 600/937                   D_loss: 1.5921 G_loss: 0.3977\n",
      "Epoch 39/50 Batch 700/937                   D_loss: 1.7463 G_loss: 0.3963\n",
      "Epoch 39/50 Batch 800/937                   D_loss: 1.9958 G_loss: 0.7535\n",
      "Epoch 39/50 Batch 900/937                   D_loss: 1.4904 G_loss: 0.5126\n",
      "Epoch 40/50 Batch 0/937                   D_loss: 1.6542 G_loss: 0.4068\n",
      "Epoch 40/50 Batch 100/937                   D_loss: 1.9775 G_loss: 0.4805\n",
      "Epoch 40/50 Batch 200/937                   D_loss: 1.6154 G_loss: 0.4739\n",
      "Epoch 40/50 Batch 300/937                   D_loss: 1.6662 G_loss: 0.5752\n",
      "Epoch 40/50 Batch 400/937                   D_loss: 1.5521 G_loss: 0.4560\n",
      "Epoch 40/50 Batch 500/937                   D_loss: 1.6715 G_loss: 0.3557\n",
      "Epoch 40/50 Batch 600/937                   D_loss: 1.5641 G_loss: 0.3562\n",
      "Epoch 40/50 Batch 700/937                   D_loss: 1.7521 G_loss: 0.5037\n",
      "Epoch 40/50 Batch 800/937                   D_loss: 1.6522 G_loss: 0.4642\n",
      "Epoch 40/50 Batch 900/937                   D_loss: 1.5040 G_loss: 0.5830\n",
      "Epoch 41/50 Batch 0/937                   D_loss: 1.5237 G_loss: 0.4109\n",
      "Epoch 41/50 Batch 100/937                   D_loss: 1.7396 G_loss: 0.5010\n",
      "Epoch 41/50 Batch 200/937                   D_loss: 1.6808 G_loss: 0.3863\n",
      "Epoch 41/50 Batch 300/937                   D_loss: 1.5747 G_loss: 0.3647\n",
      "Epoch 41/50 Batch 400/937                   D_loss: 1.7967 G_loss: 0.4639\n",
      "Epoch 41/50 Batch 500/937                   D_loss: 1.3911 G_loss: 0.5656\n",
      "Epoch 41/50 Batch 600/937                   D_loss: 1.7002 G_loss: 0.4214\n",
      "Epoch 41/50 Batch 700/937                   D_loss: 1.6669 G_loss: 0.3103\n",
      "Epoch 41/50 Batch 800/937                   D_loss: 1.5317 G_loss: 0.5889\n",
      "Epoch 41/50 Batch 900/937                   D_loss: 1.6270 G_loss: 0.4440\n",
      "Epoch 42/50 Batch 0/937                   D_loss: 1.5344 G_loss: 0.4155\n",
      "Epoch 42/50 Batch 100/937                   D_loss: 1.5463 G_loss: 0.4866\n",
      "Epoch 42/50 Batch 200/937                   D_loss: 1.4948 G_loss: 0.3663\n",
      "Epoch 42/50 Batch 300/937                   D_loss: 2.1411 G_loss: 0.5056\n",
      "Epoch 42/50 Batch 400/937                   D_loss: 1.5504 G_loss: 0.4881\n",
      "Epoch 42/50 Batch 500/937                   D_loss: 1.6377 G_loss: 0.3106\n",
      "Epoch 42/50 Batch 600/937                   D_loss: 1.6348 G_loss: 0.4178\n",
      "Epoch 42/50 Batch 700/937                   D_loss: 1.5822 G_loss: 0.4622\n",
      "Epoch 42/50 Batch 800/937                   D_loss: 1.5230 G_loss: 0.5400\n",
      "Epoch 42/50 Batch 900/937                   D_loss: 1.6387 G_loss: 0.3741\n",
      "Epoch 43/50 Batch 0/937                   D_loss: 1.6057 G_loss: 0.3845\n",
      "Epoch 43/50 Batch 100/937                   D_loss: 1.6175 G_loss: 0.3981\n",
      "Epoch 43/50 Batch 200/937                   D_loss: 1.6218 G_loss: 0.5145\n",
      "Epoch 43/50 Batch 300/937                   D_loss: 1.6742 G_loss: 0.4448\n",
      "Epoch 43/50 Batch 400/937                   D_loss: 1.6382 G_loss: 0.4419\n",
      "Epoch 43/50 Batch 500/937                   D_loss: 1.5488 G_loss: 0.3999\n",
      "Epoch 43/50 Batch 600/937                   D_loss: 1.5051 G_loss: 0.5114\n",
      "Epoch 43/50 Batch 700/937                   D_loss: 1.4568 G_loss: 0.4747\n",
      "Epoch 43/50 Batch 800/937                   D_loss: 1.6426 G_loss: 0.3892\n",
      "Epoch 43/50 Batch 900/937                   D_loss: 1.7116 G_loss: 0.4814\n",
      "Epoch 44/50 Batch 0/937                   D_loss: 1.9146 G_loss: 0.3276\n",
      "Epoch 44/50 Batch 100/937                   D_loss: 1.9208 G_loss: 0.5788\n",
      "Epoch 44/50 Batch 200/937                   D_loss: 1.5239 G_loss: 0.4703\n",
      "Epoch 44/50 Batch 300/937                   D_loss: 1.5369 G_loss: 0.4387\n",
      "Epoch 44/50 Batch 400/937                   D_loss: 1.6715 G_loss: 0.7234\n",
      "Epoch 44/50 Batch 500/937                   D_loss: 1.4797 G_loss: 0.6121\n",
      "Epoch 44/50 Batch 600/937                   D_loss: 1.5820 G_loss: 0.4480\n",
      "Epoch 44/50 Batch 700/937                   D_loss: 1.5355 G_loss: 0.4809\n",
      "Epoch 44/50 Batch 800/937                   D_loss: 1.6804 G_loss: 0.4050\n",
      "Epoch 44/50 Batch 900/937                   D_loss: 1.4657 G_loss: 0.3649\n",
      "Epoch 45/50 Batch 0/937                   D_loss: 1.6052 G_loss: 0.6483\n",
      "Epoch 45/50 Batch 100/937                   D_loss: 1.6426 G_loss: 0.4731\n",
      "Epoch 45/50 Batch 200/937                   D_loss: 1.6187 G_loss: 0.2447\n",
      "Epoch 45/50 Batch 300/937                   D_loss: 1.5604 G_loss: 0.4830\n",
      "Epoch 45/50 Batch 400/937                   D_loss: 1.5493 G_loss: 0.4492\n",
      "Epoch 45/50 Batch 500/937                   D_loss: 1.4399 G_loss: 0.4366\n",
      "Epoch 45/50 Batch 600/937                   D_loss: 1.6356 G_loss: 0.9498\n",
      "Epoch 45/50 Batch 700/937                   D_loss: 1.4157 G_loss: 0.5770\n",
      "Epoch 45/50 Batch 800/937                   D_loss: 1.5735 G_loss: 0.5251\n",
      "Epoch 45/50 Batch 900/937                   D_loss: 1.3837 G_loss: 0.3837\n",
      "Epoch 46/50 Batch 0/937                   D_loss: 1.9185 G_loss: 0.3483\n",
      "Epoch 46/50 Batch 100/937                   D_loss: 1.4481 G_loss: 0.5758\n",
      "Epoch 46/50 Batch 200/937                   D_loss: 1.6712 G_loss: 0.6891\n",
      "Epoch 46/50 Batch 300/937                   D_loss: 1.5207 G_loss: 0.4117\n",
      "Epoch 46/50 Batch 400/937                   D_loss: 1.9502 G_loss: 0.5947\n",
      "Epoch 46/50 Batch 500/937                   D_loss: 1.5915 G_loss: 0.5237\n",
      "Epoch 46/50 Batch 600/937                   D_loss: 1.6661 G_loss: 0.2810\n",
      "Epoch 46/50 Batch 700/937                   D_loss: 1.5643 G_loss: 0.3751\n",
      "Epoch 46/50 Batch 800/937                   D_loss: 1.7005 G_loss: 0.5366\n",
      "Epoch 46/50 Batch 900/937                   D_loss: 1.8335 G_loss: 0.5964\n",
      "Epoch 47/50 Batch 0/937                   D_loss: 1.6180 G_loss: 0.6230\n",
      "Epoch 47/50 Batch 100/937                   D_loss: 1.6477 G_loss: 0.4963\n",
      "Epoch 47/50 Batch 200/937                   D_loss: 1.6864 G_loss: 0.5554\n",
      "Epoch 47/50 Batch 300/937                   D_loss: 1.8167 G_loss: 0.5405\n",
      "Epoch 47/50 Batch 400/937                   D_loss: 1.5821 G_loss: 0.3117\n",
      "Epoch 47/50 Batch 500/937                   D_loss: 1.4403 G_loss: 0.6109\n",
      "Epoch 47/50 Batch 600/937                   D_loss: 1.4972 G_loss: 0.7839\n",
      "Epoch 47/50 Batch 700/937                   D_loss: 1.6643 G_loss: 0.4596\n",
      "Epoch 47/50 Batch 800/937                   D_loss: 1.5969 G_loss: 0.5621\n",
      "Epoch 47/50 Batch 900/937                   D_loss: 1.5794 G_loss: 0.8828\n",
      "Epoch 48/50 Batch 0/937                   D_loss: 1.8918 G_loss: 0.4346\n",
      "Epoch 48/50 Batch 100/937                   D_loss: 1.4515 G_loss: 0.5709\n",
      "Epoch 48/50 Batch 200/937                   D_loss: 1.7158 G_loss: 0.3357\n",
      "Epoch 48/50 Batch 300/937                   D_loss: 1.6021 G_loss: 0.4184\n",
      "Epoch 48/50 Batch 400/937                   D_loss: 1.7902 G_loss: 0.3917\n",
      "Epoch 48/50 Batch 500/937                   D_loss: 2.2814 G_loss: 0.6382\n",
      "Epoch 48/50 Batch 600/937                   D_loss: 1.7698 G_loss: 0.3375\n",
      "Epoch 48/50 Batch 700/937                   D_loss: 1.6658 G_loss: 0.5659\n",
      "Epoch 48/50 Batch 800/937                   D_loss: 1.7111 G_loss: 0.3913\n",
      "Epoch 48/50 Batch 900/937                   D_loss: 1.5085 G_loss: 0.7533\n",
      "Epoch 49/50 Batch 0/937                   D_loss: 1.5458 G_loss: 0.5610\n",
      "Epoch 49/50 Batch 100/937                   D_loss: 1.4686 G_loss: 0.4352\n",
      "Epoch 49/50 Batch 200/937                   D_loss: 1.7651 G_loss: 0.3551\n",
      "Epoch 49/50 Batch 300/937                   D_loss: 1.7114 G_loss: 0.3778\n",
      "Epoch 49/50 Batch 400/937                   D_loss: 1.6417 G_loss: 0.5539\n",
      "Epoch 49/50 Batch 500/937                   D_loss: 1.6443 G_loss: 0.2415\n",
      "Epoch 49/50 Batch 600/937                   D_loss: 1.5507 G_loss: 0.4605\n",
      "Epoch 49/50 Batch 700/937                   D_loss: 1.5099 G_loss: 0.5326\n",
      "Epoch 49/50 Batch 800/937                   D_loss: 1.8150 G_loss: 0.2777\n",
      "Epoch 49/50 Batch 900/937                   D_loss: 1.5887 G_loss: 0.5749\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, real_labels ) in enumerate(trainloader):\n",
    "        batch_size = imgs.shape[0]\n",
    "        imgs = imgs.to(device)\n",
    "        real_labels = real_labels.to(device)\n",
    "        real_labels = to_categorical(real_labels, n_classes).to(device)\n",
    "        #train discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        #real\n",
    "        code_input = torch.randn(batch_size, code_dim).to(device)\n",
    "        real_validity, real_label, real_code = discriminator(imgs)\n",
    "        d_adv_real_loss = adversarial_loss(real_validity, torch.ones_like(real_validity))\n",
    "        d_aux_real_loss = auxiliary_loss(real_label, torch.argmax(real_labels, 1))\n",
    "        d_con_real_loss = continuous_loss(real_code, code_input)\n",
    "        d_real_loss = d_adv_real_loss + d_aux_real_loss + d_con_real_loss\n",
    "        #fake\n",
    "        noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_labels = torch.randint(0, 10, (batch_size,)).to(device)\n",
    "        fake_labels = to_categorical(fake_labels, n_classes).to(device)\n",
    "        fake_imgs = generator(noise, fake_labels, code_input)\n",
    "        fake_validity, fake_label, fake_code = discriminator(fake_imgs.detach())\n",
    "        d_adv_fake_loss = adversarial_loss(fake_validity, torch.zeros_like(fake_validity))\n",
    "        d_aux_fake_loss = auxiliary_loss(fake_label, torch.argmax(fake_labels, 1))\n",
    "        d_con_fake_loss = continuous_loss(fake_code, code_input)\n",
    "        d_fake_loss = d_adv_fake_loss + d_aux_fake_loss + d_con_fake_loss\n",
    "        #total\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        #train generator\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_validity, fake_label, fake_code = discriminator(fake_imgs)\n",
    "        g_adv_loss = adversarial_loss(fake_validity, torch.ones_like(fake_validity))\n",
    "        g_aux_loss = auxiliary_loss(fake_label, torch.argmax(fake_labels, 1))\n",
    "        g_con_loss = continuous_loss(fake_code, code_input)\n",
    "        g_loss = g_adv_loss + g_aux_loss + g_con_loss\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        #print\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{n_epochs} Batch {i}/{len(trainloader)} \\\n",
    "                  D_loss: {d_loss.item():.4f} G_loss: {g_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate images\n",
    "noise = torch.randn(64, latent_dim).to(device)\n",
    "labels = torch.randint(0, 2, (64,)).to(device)\n",
    "labels = to_categorical(labels, n_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = torch.randn(64, code_dim).to(device)\n",
    "gen_imgs = generator(noise, labels, code)\n",
    "gen_imgs = gen_imgs.view(gen_imgs.shape[0], 1, 28, 28)\n",
    "gen_imgs = gen_imgs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1, 28, 28)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot 3x3\n",
    "def plot_images(imgs):\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    cnt = 0\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, 0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(gen_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77f0ac4a1ff910c6f832be6ab53afe92115f75471ff7ffff1273b50351d0e386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
