{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Defination of Embedding Layer:\n",
    "```\n",
    "class torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "- num_embeddings (int) – size of the dictionary of embeddings\n",
    "\n",
    "- embedding_dim (int) – the size of each embedding vector\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's initialize the embedding layer with random weights:\n",
    "\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "emb = nn.Embedding(10, 3)\n",
    "print(emb.weight)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.7113, -0.1592, -1.9761],\n",
      "        [ 0.7417, -0.6407, -0.9271],\n",
      "        [-0.4729,  0.6831, -0.5181],\n",
      "        [-1.4853,  0.2756, -1.0121],\n",
      "        [ 1.3261, -1.3668, -1.1355],\n",
      "        [ 1.6273,  0.6024, -0.9206],\n",
      "        [-0.9628, -0.4562, -0.0948],\n",
      "        [ 1.5996, -1.5538, -0.4515],\n",
      "        [-0.3031,  1.9738,  0.5651],\n",
      "        [ 0.1933,  0.4819, -1.1980]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "emb = nn.Embedding(10, 3)\n",
    "print(emb.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```So, what it means, is that we have a dictionary of 10 words or anything that you might want to encode, and each word is represented by a vector of size 3. So, the size of the embedding layer is 10x3.```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Now, let's see how the embedding layer works. We will create a tensor of indices and pass it to the embedding layer. The output will be the embedding vector for each index.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randint(0, 10, (10, ))\n",
    "print(X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "So, we have a tensor of size 10, which is 10 rows. Now, each of the number in X will be represented by a vector of size 3. So, the output will be a tensor of size 10x3.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "out = emb(X)\n",
    "print(out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "3 is important there, as this will pertain no matter that the input size is. If the input size is, let's say, 5, then the output will be 5x3. So, the output size will always be the input size x the embedding size.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randint(0, 10, (5, ))\n",
    "print(X.shape)\n",
    "\n",
    "out = emb(X)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Which means, in our embedding, we can encode 10 numbers at most, but we are encoding 5 in this specific case. So, the output will be 5x3.\n",
    "And all of these 5 numbers are in the range of 0 and 9. Our embedding also can fit at most 10 numbers.\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Now, let's see how the weights look like\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.7113, -0.1592, -1.9761],\n",
      "        [ 0.7417, -0.6407, -0.9271],\n",
      "        [-0.4729,  0.6831, -0.5181],\n",
      "        [-1.4853,  0.2756, -1.0121],\n",
      "        [ 1.3261, -1.3668, -1.1355],\n",
      "        [ 1.6273,  0.6024, -0.9206],\n",
      "        [-0.9628, -0.4562, -0.0948],\n",
      "        [ 1.5996, -1.5538, -0.4515],\n",
      "        [-0.3031,  1.9738,  0.5651],\n",
      "        [ 0.1933,  0.4819, -1.1980]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(emb.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7113, -0.1592, -1.9761],\n",
      "        [ 0.7417, -0.6407, -0.9271]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[-0.4729,  0.6831, -0.5181],\n",
      "        [-1.4853,  0.2756, -1.0121],\n",
      "        [ 1.3261, -1.3668, -1.1355]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(emb(torch.tensor([0,1])))\n",
    "print(emb(torch.tensor([2, 3, 4])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "So, what if we want to encode a number bigger than 9? Can our embedding layer do that?\n",
    "As we can see from above prints, each element of in rows are assigned to numbers from 0 to 9. So, if we pass a number bigger than 9, it will throw an error.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "print(emb(torch.tensor([10])))\n",
    "this will throw an error: IndexError: index out of range in self\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77f0ac4a1ff910c6f832be6ab53afe92115f75471ff7ffff1273b50351d0e386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
