{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mnist dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))])  \n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "batch_size = 64\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, drop_last=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return F.one_hot(y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = to_categorical(y, 10)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infoGAN\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, n_classes, code_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.code_dim = code_dim\n",
    "        input_dim = latent_dim + n_classes + code_dim\n",
    "        img_size = 28\n",
    "        self.init_size = img_size // 4\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128 * self.init_size ** 2)\n",
    "        )\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 1, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels, code):\n",
    "        gen_input = torch.cat((noise, labels, code), -1)\n",
    "        img = self.l1(gen_input)\n",
    "        img = img.view(img.size(0), 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "n_classes = 10\n",
    "code_dim = 5\n",
    "generator = Generator(latent_dim, n_classes, code_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100]) torch.Size([64, 10]) torch.Size([64, 5])\n",
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "noise = torch.randn(64, latent_dim).to(device)\n",
    "labels = torch.randint(0, 10, (64,)).to(device)\n",
    "labels = to_categorical(labels, n_classes).to(device)\n",
    "code = torch.randn(64, code_dim).to(device)\n",
    "print(noise.shape, labels.shape, code.shape)\n",
    "gen_out = generator(noise, labels, code)\n",
    "print(gen_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_classes, code_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.code_dim = code_dim\n",
    "        img_size = 28\n",
    "        ds_size = img_size // 4\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(16, 32, 3, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        out = 1024\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(out, 1))\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(out, n_classes))\n",
    "        self.code_layer = nn.Sequential(nn.Linear(out, code_dim))\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "        code = self.code_layer(out)\n",
    "        return validity, label, code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1]) torch.Size([64, 10]) torch.Size([64, 5])\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator(n_classes, code_dim).to(device)\n",
    "disc_out = discriminator(gen_out)\n",
    "print(disc_out[0].shape, disc_out[1].shape, disc_out[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss functions\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "continuous_loss = torch.nn.MSELoss()\n",
    "\n",
    "#optimizer\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999  \n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50 Batch 0/937                   D_loss: 1.8385 G_loss: 0.7611\n",
      "Epoch 0/50 Batch 100/937                   D_loss: 1.9828 G_loss: 0.9238\n",
      "Epoch 0/50 Batch 200/937                   D_loss: 2.2396 G_loss: 0.4079\n",
      "Epoch 0/50 Batch 300/937                   D_loss: 1.7146 G_loss: 0.6183\n",
      "Epoch 0/50 Batch 400/937                   D_loss: 1.9043 G_loss: 0.6317\n",
      "Epoch 0/50 Batch 500/937                   D_loss: 1.8125 G_loss: 0.8579\n",
      "Epoch 0/50 Batch 600/937                   D_loss: 1.7907 G_loss: 0.6476\n",
      "Epoch 0/50 Batch 700/937                   D_loss: 1.8951 G_loss: 0.5826\n",
      "Epoch 0/50 Batch 800/937                   D_loss: 2.0103 G_loss: 0.5682\n",
      "Epoch 0/50 Batch 900/937                   D_loss: 1.8653 G_loss: 0.6135\n",
      "Epoch 1/50 Batch 0/937                   D_loss: 1.7003 G_loss: 0.8349\n",
      "Epoch 1/50 Batch 100/937                   D_loss: 1.6691 G_loss: 0.6297\n",
      "Epoch 1/50 Batch 200/937                   D_loss: 1.6369 G_loss: 0.6111\n",
      "Epoch 1/50 Batch 300/937                   D_loss: 1.6517 G_loss: 0.6344\n",
      "Epoch 1/50 Batch 400/937                   D_loss: 1.9126 G_loss: 0.7079\n",
      "Epoch 1/50 Batch 500/937                   D_loss: 1.9157 G_loss: 0.8023\n",
      "Epoch 1/50 Batch 600/937                   D_loss: 1.9219 G_loss: 0.6623\n",
      "Epoch 1/50 Batch 700/937                   D_loss: 1.7475 G_loss: 0.6055\n",
      "Epoch 1/50 Batch 800/937                   D_loss: 1.7893 G_loss: 0.6977\n",
      "Epoch 1/50 Batch 900/937                   D_loss: 1.8760 G_loss: 0.7353\n",
      "Epoch 2/50 Batch 0/937                   D_loss: 1.8862 G_loss: 0.6047\n",
      "Epoch 2/50 Batch 100/937                   D_loss: 1.5823 G_loss: 0.6132\n",
      "Epoch 2/50 Batch 200/937                   D_loss: 1.7080 G_loss: 0.7653\n",
      "Epoch 2/50 Batch 300/937                   D_loss: 1.8079 G_loss: 0.6087\n",
      "Epoch 2/50 Batch 400/937                   D_loss: 1.7455 G_loss: 0.8096\n",
      "Epoch 2/50 Batch 500/937                   D_loss: 1.7040 G_loss: 0.7088\n",
      "Epoch 2/50 Batch 600/937                   D_loss: 1.8081 G_loss: 0.6089\n",
      "Epoch 2/50 Batch 700/937                   D_loss: 1.7243 G_loss: 0.6914\n",
      "Epoch 2/50 Batch 800/937                   D_loss: 1.7893 G_loss: 0.6326\n",
      "Epoch 2/50 Batch 900/937                   D_loss: 1.9432 G_loss: 0.7259\n",
      "Epoch 3/50 Batch 0/937                   D_loss: 1.9335 G_loss: 0.6444\n",
      "Epoch 3/50 Batch 100/937                   D_loss: 1.7899 G_loss: 0.6868\n",
      "Epoch 3/50 Batch 200/937                   D_loss: 1.9320 G_loss: 0.5932\n",
      "Epoch 3/50 Batch 300/937                   D_loss: 1.7594 G_loss: 0.6516\n",
      "Epoch 3/50 Batch 400/937                   D_loss: 1.9867 G_loss: 0.6468\n",
      "Epoch 3/50 Batch 500/937                   D_loss: 1.6562 G_loss: 0.5738\n",
      "Epoch 3/50 Batch 600/937                   D_loss: 1.8150 G_loss: 0.7019\n",
      "Epoch 3/50 Batch 700/937                   D_loss: 1.7562 G_loss: 0.6646\n",
      "Epoch 3/50 Batch 800/937                   D_loss: 1.7233 G_loss: 0.6693\n",
      "Epoch 3/50 Batch 900/937                   D_loss: 1.7246 G_loss: 0.6819\n",
      "Epoch 4/50 Batch 0/937                   D_loss: 1.8746 G_loss: 0.4732\n",
      "Epoch 4/50 Batch 100/937                   D_loss: 1.6200 G_loss: 0.6671\n",
      "Epoch 4/50 Batch 200/937                   D_loss: 1.8201 G_loss: 0.4607\n",
      "Epoch 4/50 Batch 300/937                   D_loss: 1.5899 G_loss: 0.6621\n",
      "Epoch 4/50 Batch 400/937                   D_loss: 1.5540 G_loss: 0.5798\n",
      "Epoch 4/50 Batch 500/937                   D_loss: 1.8764 G_loss: 0.4082\n",
      "Epoch 4/50 Batch 600/937                   D_loss: 1.5715 G_loss: 0.7068\n",
      "Epoch 4/50 Batch 700/937                   D_loss: 1.9185 G_loss: 0.4636\n",
      "Epoch 4/50 Batch 800/937                   D_loss: 1.7084 G_loss: 0.4723\n",
      "Epoch 4/50 Batch 900/937                   D_loss: 1.7176 G_loss: 0.6036\n",
      "Epoch 5/50 Batch 0/937                   D_loss: 1.7165 G_loss: 0.6516\n",
      "Epoch 5/50 Batch 100/937                   D_loss: 1.7406 G_loss: 0.6082\n",
      "Epoch 5/50 Batch 200/937                   D_loss: 1.5914 G_loss: 0.5457\n",
      "Epoch 5/50 Batch 300/937                   D_loss: 1.6676 G_loss: 0.5191\n",
      "Epoch 5/50 Batch 400/937                   D_loss: 1.8389 G_loss: 0.6541\n",
      "Epoch 5/50 Batch 500/937                   D_loss: 1.8906 G_loss: 0.5502\n",
      "Epoch 5/50 Batch 600/937                   D_loss: 1.7481 G_loss: 0.6036\n",
      "Epoch 5/50 Batch 700/937                   D_loss: 1.7026 G_loss: 0.5904\n",
      "Epoch 5/50 Batch 800/937                   D_loss: 1.9027 G_loss: 0.7869\n",
      "Epoch 5/50 Batch 900/937                   D_loss: 1.7451 G_loss: 0.5761\n",
      "Epoch 6/50 Batch 0/937                   D_loss: 1.8526 G_loss: 0.4554\n",
      "Epoch 6/50 Batch 100/937                   D_loss: 1.8423 G_loss: 0.5396\n",
      "Epoch 6/50 Batch 200/937                   D_loss: 1.6735 G_loss: 0.5238\n",
      "Epoch 6/50 Batch 300/937                   D_loss: 1.8464 G_loss: 0.6449\n",
      "Epoch 6/50 Batch 400/937                   D_loss: 1.7817 G_loss: 0.6024\n",
      "Epoch 6/50 Batch 500/937                   D_loss: 1.7549 G_loss: 0.6271\n",
      "Epoch 6/50 Batch 600/937                   D_loss: 1.7258 G_loss: 0.4032\n",
      "Epoch 6/50 Batch 700/937                   D_loss: 1.6741 G_loss: 0.4833\n",
      "Epoch 6/50 Batch 800/937                   D_loss: 1.6628 G_loss: 0.7343\n",
      "Epoch 6/50 Batch 900/937                   D_loss: 1.7493 G_loss: 0.4436\n",
      "Epoch 7/50 Batch 0/937                   D_loss: 1.7661 G_loss: 0.4611\n",
      "Epoch 7/50 Batch 100/937                   D_loss: 1.8160 G_loss: 0.5976\n",
      "Epoch 7/50 Batch 200/937                   D_loss: 1.7549 G_loss: 0.4898\n",
      "Epoch 7/50 Batch 300/937                   D_loss: 1.6515 G_loss: 0.6422\n",
      "Epoch 7/50 Batch 400/937                   D_loss: 1.6813 G_loss: 0.5322\n",
      "Epoch 7/50 Batch 500/937                   D_loss: 1.6059 G_loss: 0.7259\n",
      "Epoch 7/50 Batch 600/937                   D_loss: 1.7106 G_loss: 0.4835\n",
      "Epoch 7/50 Batch 700/937                   D_loss: 1.7514 G_loss: 0.6032\n",
      "Epoch 7/50 Batch 800/937                   D_loss: 1.6087 G_loss: 0.5318\n",
      "Epoch 7/50 Batch 900/937                   D_loss: 1.6912 G_loss: 0.5522\n",
      "Epoch 8/50 Batch 0/937                   D_loss: 1.7199 G_loss: 0.5093\n",
      "Epoch 8/50 Batch 100/937                   D_loss: 1.8154 G_loss: 0.6656\n",
      "Epoch 8/50 Batch 200/937                   D_loss: 1.6757 G_loss: 0.6105\n",
      "Epoch 8/50 Batch 300/937                   D_loss: 1.6051 G_loss: 0.5011\n",
      "Epoch 8/50 Batch 400/937                   D_loss: 1.6537 G_loss: 0.5960\n",
      "Epoch 8/50 Batch 500/937                   D_loss: 1.8248 G_loss: 0.4058\n",
      "Epoch 8/50 Batch 600/937                   D_loss: 1.8816 G_loss: 0.4110\n",
      "Epoch 8/50 Batch 700/937                   D_loss: 1.4738 G_loss: 0.6490\n",
      "Epoch 8/50 Batch 800/937                   D_loss: 1.7814 G_loss: 0.6450\n",
      "Epoch 8/50 Batch 900/937                   D_loss: 1.9687 G_loss: 0.5243\n",
      "Epoch 9/50 Batch 0/937                   D_loss: 1.5548 G_loss: 0.5491\n",
      "Epoch 9/50 Batch 100/937                   D_loss: 1.8692 G_loss: 0.5055\n",
      "Epoch 9/50 Batch 200/937                   D_loss: 1.5995 G_loss: 0.4238\n",
      "Epoch 9/50 Batch 300/937                   D_loss: 1.5798 G_loss: 0.4730\n",
      "Epoch 9/50 Batch 400/937                   D_loss: 1.7712 G_loss: 0.5470\n",
      "Epoch 9/50 Batch 500/937                   D_loss: 1.5635 G_loss: 0.5741\n",
      "Epoch 9/50 Batch 600/937                   D_loss: 1.8230 G_loss: 0.5692\n",
      "Epoch 9/50 Batch 700/937                   D_loss: 1.6650 G_loss: 0.7192\n",
      "Epoch 9/50 Batch 800/937                   D_loss: 1.6120 G_loss: 0.4493\n",
      "Epoch 9/50 Batch 900/937                   D_loss: 1.4992 G_loss: 0.5193\n",
      "Epoch 10/50 Batch 0/937                   D_loss: 1.9590 G_loss: 0.5154\n",
      "Epoch 10/50 Batch 100/937                   D_loss: 1.8241 G_loss: 0.4082\n",
      "Epoch 10/50 Batch 200/937                   D_loss: 1.5781 G_loss: 0.5902\n",
      "Epoch 10/50 Batch 300/937                   D_loss: 1.6502 G_loss: 0.7162\n",
      "Epoch 10/50 Batch 400/937                   D_loss: 1.6895 G_loss: 0.5937\n",
      "Epoch 10/50 Batch 500/937                   D_loss: 1.8050 G_loss: 0.5291\n",
      "Epoch 10/50 Batch 600/937                   D_loss: 1.5905 G_loss: 0.6267\n",
      "Epoch 10/50 Batch 700/937                   D_loss: 1.8538 G_loss: 0.5160\n",
      "Epoch 10/50 Batch 800/937                   D_loss: 1.9767 G_loss: 0.5444\n",
      "Epoch 10/50 Batch 900/937                   D_loss: 1.6375 G_loss: 0.5477\n",
      "Epoch 11/50 Batch 0/937                   D_loss: 1.9330 G_loss: 0.5817\n",
      "Epoch 11/50 Batch 100/937                   D_loss: 1.9458 G_loss: 0.4895\n",
      "Epoch 11/50 Batch 200/937                   D_loss: 1.6812 G_loss: 0.5770\n",
      "Epoch 11/50 Batch 300/937                   D_loss: 1.5855 G_loss: 0.5538\n",
      "Epoch 11/50 Batch 400/937                   D_loss: 1.5205 G_loss: 0.6606\n",
      "Epoch 11/50 Batch 500/937                   D_loss: 1.5929 G_loss: 0.4802\n",
      "Epoch 11/50 Batch 600/937                   D_loss: 1.8261 G_loss: 0.5538\n",
      "Epoch 11/50 Batch 700/937                   D_loss: 1.6242 G_loss: 0.4914\n",
      "Epoch 11/50 Batch 800/937                   D_loss: 1.8805 G_loss: 0.5274\n",
      "Epoch 11/50 Batch 900/937                   D_loss: 1.8264 G_loss: 0.4996\n",
      "Epoch 12/50 Batch 0/937                   D_loss: 1.7616 G_loss: 0.5441\n",
      "Epoch 12/50 Batch 100/937                   D_loss: 1.5705 G_loss: 0.4647\n",
      "Epoch 12/50 Batch 200/937                   D_loss: 1.7918 G_loss: 0.5337\n",
      "Epoch 12/50 Batch 300/937                   D_loss: 1.5448 G_loss: 0.5873\n",
      "Epoch 12/50 Batch 400/937                   D_loss: 1.4621 G_loss: 0.7667\n",
      "Epoch 12/50 Batch 500/937                   D_loss: 1.6607 G_loss: 0.4792\n",
      "Epoch 12/50 Batch 600/937                   D_loss: 1.8142 G_loss: 0.6966\n",
      "Epoch 12/50 Batch 700/937                   D_loss: 1.5045 G_loss: 0.5716\n",
      "Epoch 12/50 Batch 800/937                   D_loss: 1.7505 G_loss: 0.5608\n",
      "Epoch 12/50 Batch 900/937                   D_loss: 1.5235 G_loss: 0.4219\n",
      "Epoch 13/50 Batch 0/937                   D_loss: 1.8267 G_loss: 0.4497\n",
      "Epoch 13/50 Batch 100/937                   D_loss: 1.5940 G_loss: 0.5727\n",
      "Epoch 13/50 Batch 200/937                   D_loss: 1.5920 G_loss: 0.6159\n",
      "Epoch 13/50 Batch 300/937                   D_loss: 1.8918 G_loss: 0.6134\n",
      "Epoch 13/50 Batch 400/937                   D_loss: 1.7394 G_loss: 0.6005\n",
      "Epoch 13/50 Batch 500/937                   D_loss: 1.6708 G_loss: 0.6528\n",
      "Epoch 13/50 Batch 600/937                   D_loss: 1.9930 G_loss: 0.5834\n",
      "Epoch 13/50 Batch 700/937                   D_loss: 1.6403 G_loss: 0.4291\n",
      "Epoch 13/50 Batch 800/937                   D_loss: 1.8310 G_loss: 0.5016\n",
      "Epoch 13/50 Batch 900/937                   D_loss: 1.7439 G_loss: 0.5266\n",
      "Epoch 14/50 Batch 0/937                   D_loss: 1.6220 G_loss: 0.5902\n",
      "Epoch 14/50 Batch 100/937                   D_loss: 1.7757 G_loss: 0.4944\n",
      "Epoch 14/50 Batch 200/937                   D_loss: 1.5375 G_loss: 0.4891\n",
      "Epoch 14/50 Batch 300/937                   D_loss: 1.6590 G_loss: 0.5093\n",
      "Epoch 14/50 Batch 400/937                   D_loss: 1.7855 G_loss: 0.5425\n",
      "Epoch 14/50 Batch 500/937                   D_loss: 1.6436 G_loss: 0.5291\n",
      "Epoch 14/50 Batch 600/937                   D_loss: 1.7132 G_loss: 0.5947\n",
      "Epoch 14/50 Batch 700/937                   D_loss: 1.7570 G_loss: 0.5484\n",
      "Epoch 14/50 Batch 800/937                   D_loss: 1.8936 G_loss: 0.4762\n",
      "Epoch 14/50 Batch 900/937                   D_loss: 1.6342 G_loss: 0.5844\n",
      "Epoch 15/50 Batch 0/937                   D_loss: 1.9427 G_loss: 0.6590\n",
      "Epoch 15/50 Batch 100/937                   D_loss: 2.0344 G_loss: 0.6078\n",
      "Epoch 15/50 Batch 200/937                   D_loss: 1.6310 G_loss: 0.5369\n",
      "Epoch 15/50 Batch 300/937                   D_loss: 1.6558 G_loss: 0.4287\n",
      "Epoch 15/50 Batch 400/937                   D_loss: 1.6591 G_loss: 0.6959\n",
      "Epoch 15/50 Batch 500/937                   D_loss: 1.7960 G_loss: 0.4687\n",
      "Epoch 15/50 Batch 600/937                   D_loss: 1.6369 G_loss: 0.6859\n",
      "Epoch 15/50 Batch 700/937                   D_loss: 1.8499 G_loss: 0.4259\n",
      "Epoch 15/50 Batch 800/937                   D_loss: 1.6519 G_loss: 0.5154\n",
      "Epoch 15/50 Batch 900/937                   D_loss: 1.8241 G_loss: 0.4523\n",
      "Epoch 16/50 Batch 0/937                   D_loss: 1.7271 G_loss: 0.4233\n",
      "Epoch 16/50 Batch 100/937                   D_loss: 1.6833 G_loss: 0.4449\n",
      "Epoch 16/50 Batch 200/937                   D_loss: 1.6969 G_loss: 0.4937\n",
      "Epoch 16/50 Batch 300/937                   D_loss: 1.7954 G_loss: 0.5752\n",
      "Epoch 16/50 Batch 400/937                   D_loss: 1.7377 G_loss: 0.6065\n",
      "Epoch 16/50 Batch 500/937                   D_loss: 1.9013 G_loss: 0.3811\n",
      "Epoch 16/50 Batch 600/937                   D_loss: 1.9537 G_loss: 0.5168\n",
      "Epoch 16/50 Batch 700/937                   D_loss: 1.7674 G_loss: 0.5017\n",
      "Epoch 16/50 Batch 800/937                   D_loss: 1.7871 G_loss: 0.4541\n",
      "Epoch 16/50 Batch 900/937                   D_loss: 1.7945 G_loss: 0.5503\n",
      "Epoch 17/50 Batch 0/937                   D_loss: 1.8290 G_loss: 0.4902\n",
      "Epoch 17/50 Batch 100/937                   D_loss: 1.8113 G_loss: 0.5439\n",
      "Epoch 17/50 Batch 200/937                   D_loss: 1.8490 G_loss: 0.4553\n",
      "Epoch 17/50 Batch 300/937                   D_loss: 1.8145 G_loss: 0.5386\n",
      "Epoch 17/50 Batch 400/937                   D_loss: 1.7256 G_loss: 0.4633\n",
      "Epoch 17/50 Batch 500/937                   D_loss: 1.8174 G_loss: 0.6684\n",
      "Epoch 17/50 Batch 600/937                   D_loss: 1.6046 G_loss: 0.6383\n",
      "Epoch 17/50 Batch 700/937                   D_loss: 1.9428 G_loss: 0.3379\n",
      "Epoch 17/50 Batch 800/937                   D_loss: 1.6423 G_loss: 0.5575\n",
      "Epoch 17/50 Batch 900/937                   D_loss: 1.5267 G_loss: 0.4398\n",
      "Epoch 18/50 Batch 0/937                   D_loss: 1.7549 G_loss: 0.4355\n",
      "Epoch 18/50 Batch 100/937                   D_loss: 1.7820 G_loss: 0.4498\n",
      "Epoch 18/50 Batch 200/937                   D_loss: 1.5735 G_loss: 0.4483\n",
      "Epoch 18/50 Batch 300/937                   D_loss: 1.7169 G_loss: 0.7651\n",
      "Epoch 18/50 Batch 400/937                   D_loss: 1.5639 G_loss: 0.5903\n",
      "Epoch 18/50 Batch 500/937                   D_loss: 1.7483 G_loss: 0.5092\n",
      "Epoch 18/50 Batch 600/937                   D_loss: 1.6958 G_loss: 0.5158\n",
      "Epoch 18/50 Batch 700/937                   D_loss: 1.7321 G_loss: 0.5250\n",
      "Epoch 18/50 Batch 800/937                   D_loss: 1.6204 G_loss: 0.5587\n",
      "Epoch 18/50 Batch 900/937                   D_loss: 1.6661 G_loss: 0.4320\n",
      "Epoch 19/50 Batch 0/937                   D_loss: 1.7253 G_loss: 0.4376\n",
      "Epoch 19/50 Batch 100/937                   D_loss: 1.7787 G_loss: 0.4323\n",
      "Epoch 19/50 Batch 200/937                   D_loss: 1.7004 G_loss: 0.3931\n",
      "Epoch 19/50 Batch 300/937                   D_loss: 1.7852 G_loss: 0.5128\n",
      "Epoch 19/50 Batch 400/937                   D_loss: 1.5658 G_loss: 0.6411\n",
      "Epoch 19/50 Batch 500/937                   D_loss: 1.7880 G_loss: 0.6459\n",
      "Epoch 19/50 Batch 600/937                   D_loss: 1.8846 G_loss: 0.5043\n",
      "Epoch 19/50 Batch 700/937                   D_loss: 1.7941 G_loss: 0.4620\n",
      "Epoch 19/50 Batch 800/937                   D_loss: 1.7234 G_loss: 0.6056\n",
      "Epoch 19/50 Batch 900/937                   D_loss: 1.7731 G_loss: 0.5218\n",
      "Epoch 20/50 Batch 0/937                   D_loss: 2.0286 G_loss: 0.4641\n",
      "Epoch 20/50 Batch 100/937                   D_loss: 1.5658 G_loss: 0.6192\n",
      "Epoch 20/50 Batch 200/937                   D_loss: 1.6467 G_loss: 0.5857\n",
      "Epoch 20/50 Batch 300/937                   D_loss: 1.6409 G_loss: 0.5073\n",
      "Epoch 20/50 Batch 400/937                   D_loss: 1.7589 G_loss: 0.4078\n",
      "Epoch 20/50 Batch 500/937                   D_loss: 1.7826 G_loss: 0.5710\n",
      "Epoch 20/50 Batch 600/937                   D_loss: 1.5517 G_loss: 0.5194\n",
      "Epoch 20/50 Batch 700/937                   D_loss: 1.7093 G_loss: 0.4011\n",
      "Epoch 20/50 Batch 800/937                   D_loss: 1.8350 G_loss: 0.4920\n",
      "Epoch 20/50 Batch 900/937                   D_loss: 1.8334 G_loss: 0.2692\n",
      "Epoch 21/50 Batch 0/937                   D_loss: 1.6300 G_loss: 0.3903\n",
      "Epoch 21/50 Batch 100/937                   D_loss: 1.4844 G_loss: 0.6305\n",
      "Epoch 21/50 Batch 200/937                   D_loss: 1.6568 G_loss: 0.6069\n",
      "Epoch 21/50 Batch 300/937                   D_loss: 1.5328 G_loss: 0.6206\n",
      "Epoch 21/50 Batch 400/937                   D_loss: 1.6830 G_loss: 0.5027\n",
      "Epoch 21/50 Batch 500/937                   D_loss: 1.6075 G_loss: 0.5529\n",
      "Epoch 21/50 Batch 600/937                   D_loss: 1.6317 G_loss: 0.5117\n",
      "Epoch 21/50 Batch 700/937                   D_loss: 1.6271 G_loss: 0.6270\n",
      "Epoch 21/50 Batch 800/937                   D_loss: 1.9007 G_loss: 0.3523\n",
      "Epoch 21/50 Batch 900/937                   D_loss: 1.7575 G_loss: 0.5712\n",
      "Epoch 22/50 Batch 0/937                   D_loss: 1.7807 G_loss: 0.4972\n",
      "Epoch 22/50 Batch 100/937                   D_loss: 1.6342 G_loss: 0.4419\n",
      "Epoch 22/50 Batch 200/937                   D_loss: 1.7735 G_loss: 0.4574\n",
      "Epoch 22/50 Batch 300/937                   D_loss: 1.6484 G_loss: 0.4842\n",
      "Epoch 22/50 Batch 400/937                   D_loss: 1.8175 G_loss: 0.4895\n",
      "Epoch 22/50 Batch 500/937                   D_loss: 1.6106 G_loss: 0.4477\n",
      "Epoch 22/50 Batch 600/937                   D_loss: 1.7675 G_loss: 0.4112\n",
      "Epoch 22/50 Batch 700/937                   D_loss: 1.7763 G_loss: 0.4721\n",
      "Epoch 22/50 Batch 800/937                   D_loss: 1.6039 G_loss: 0.4276\n",
      "Epoch 22/50 Batch 900/937                   D_loss: 1.4023 G_loss: 0.4476\n",
      "Epoch 23/50 Batch 0/937                   D_loss: 1.8404 G_loss: 0.5972\n",
      "Epoch 23/50 Batch 100/937                   D_loss: 1.6640 G_loss: 0.7060\n",
      "Epoch 23/50 Batch 200/937                   D_loss: 1.7833 G_loss: 0.6403\n",
      "Epoch 23/50 Batch 300/937                   D_loss: 1.7380 G_loss: 0.8381\n",
      "Epoch 23/50 Batch 400/937                   D_loss: 1.7462 G_loss: 0.5220\n",
      "Epoch 23/50 Batch 500/937                   D_loss: 1.7290 G_loss: 0.3854\n",
      "Epoch 23/50 Batch 600/937                   D_loss: 1.8917 G_loss: 0.5423\n",
      "Epoch 23/50 Batch 700/937                   D_loss: 1.7814 G_loss: 0.6124\n",
      "Epoch 23/50 Batch 800/937                   D_loss: 1.6043 G_loss: 0.5063\n",
      "Epoch 23/50 Batch 900/937                   D_loss: 1.7913 G_loss: 0.4382\n",
      "Epoch 24/50 Batch 0/937                   D_loss: 1.6641 G_loss: 0.4895\n",
      "Epoch 24/50 Batch 100/937                   D_loss: 1.5858 G_loss: 0.4733\n",
      "Epoch 24/50 Batch 200/937                   D_loss: 1.7730 G_loss: 0.5255\n",
      "Epoch 24/50 Batch 300/937                   D_loss: 1.5695 G_loss: 0.4424\n",
      "Epoch 24/50 Batch 400/937                   D_loss: 1.8777 G_loss: 0.3924\n",
      "Epoch 24/50 Batch 500/937                   D_loss: 1.5689 G_loss: 0.5268\n",
      "Epoch 24/50 Batch 600/937                   D_loss: 1.6215 G_loss: 0.4328\n",
      "Epoch 24/50 Batch 700/937                   D_loss: 1.7309 G_loss: 0.4886\n",
      "Epoch 24/50 Batch 800/937                   D_loss: 1.8565 G_loss: 0.5659\n",
      "Epoch 24/50 Batch 900/937                   D_loss: 1.6547 G_loss: 0.3573\n",
      "Epoch 25/50 Batch 0/937                   D_loss: 1.6963 G_loss: 0.5306\n",
      "Epoch 25/50 Batch 100/937                   D_loss: 1.5573 G_loss: 0.6303\n",
      "Epoch 25/50 Batch 200/937                   D_loss: 1.5577 G_loss: 0.5455\n",
      "Epoch 25/50 Batch 300/937                   D_loss: 1.6586 G_loss: 0.5436\n",
      "Epoch 25/50 Batch 400/937                   D_loss: 1.6441 G_loss: 0.5806\n",
      "Epoch 25/50 Batch 500/937                   D_loss: 1.5722 G_loss: 0.3998\n",
      "Epoch 25/50 Batch 600/937                   D_loss: 1.6880 G_loss: 0.2325\n",
      "Epoch 25/50 Batch 700/937                   D_loss: 1.5065 G_loss: 0.6873\n",
      "Epoch 25/50 Batch 800/937                   D_loss: 1.5983 G_loss: 0.4243\n",
      "Epoch 25/50 Batch 900/937                   D_loss: 1.5350 G_loss: 0.4890\n",
      "Epoch 26/50 Batch 0/937                   D_loss: 1.4986 G_loss: 0.3978\n",
      "Epoch 26/50 Batch 100/937                   D_loss: 1.8222 G_loss: 0.3596\n",
      "Epoch 26/50 Batch 200/937                   D_loss: 1.6924 G_loss: 0.5101\n",
      "Epoch 26/50 Batch 300/937                   D_loss: 1.5668 G_loss: 0.4779\n",
      "Epoch 26/50 Batch 400/937                   D_loss: 1.7609 G_loss: 0.4321\n",
      "Epoch 26/50 Batch 500/937                   D_loss: 1.5893 G_loss: 0.6183\n",
      "Epoch 26/50 Batch 600/937                   D_loss: 1.6704 G_loss: 0.5890\n",
      "Epoch 26/50 Batch 700/937                   D_loss: 1.7249 G_loss: 0.4199\n",
      "Epoch 26/50 Batch 800/937                   D_loss: 1.9231 G_loss: 0.5637\n",
      "Epoch 26/50 Batch 900/937                   D_loss: 1.7218 G_loss: 0.5206\n",
      "Epoch 27/50 Batch 0/937                   D_loss: 1.6141 G_loss: 0.5569\n",
      "Epoch 27/50 Batch 100/937                   D_loss: 1.5413 G_loss: 0.4845\n",
      "Epoch 27/50 Batch 200/937                   D_loss: 1.7146 G_loss: 0.5983\n",
      "Epoch 27/50 Batch 300/937                   D_loss: 1.6464 G_loss: 0.4173\n",
      "Epoch 27/50 Batch 400/937                   D_loss: 1.7849 G_loss: 0.5773\n",
      "Epoch 27/50 Batch 500/937                   D_loss: 1.7544 G_loss: 0.2817\n",
      "Epoch 27/50 Batch 600/937                   D_loss: 1.5180 G_loss: 0.5204\n",
      "Epoch 27/50 Batch 700/937                   D_loss: 1.7704 G_loss: 0.3967\n",
      "Epoch 27/50 Batch 800/937                   D_loss: 1.6422 G_loss: 0.5093\n",
      "Epoch 27/50 Batch 900/937                   D_loss: 1.7516 G_loss: 0.5484\n",
      "Epoch 28/50 Batch 0/937                   D_loss: 1.7132 G_loss: 0.4293\n",
      "Epoch 28/50 Batch 100/937                   D_loss: 1.7500 G_loss: 0.4166\n",
      "Epoch 28/50 Batch 200/937                   D_loss: 1.7219 G_loss: 0.3391\n",
      "Epoch 28/50 Batch 300/937                   D_loss: 1.5633 G_loss: 0.4846\n",
      "Epoch 28/50 Batch 400/937                   D_loss: 1.7050 G_loss: 0.6304\n",
      "Epoch 28/50 Batch 500/937                   D_loss: 1.7254 G_loss: 0.3800\n",
      "Epoch 28/50 Batch 600/937                   D_loss: 1.7799 G_loss: 0.4249\n",
      "Epoch 28/50 Batch 700/937                   D_loss: 1.5647 G_loss: 0.3971\n",
      "Epoch 28/50 Batch 800/937                   D_loss: 1.5050 G_loss: 0.4311\n",
      "Epoch 28/50 Batch 900/937                   D_loss: 1.5582 G_loss: 0.5411\n",
      "Epoch 29/50 Batch 0/937                   D_loss: 1.6397 G_loss: 0.4717\n",
      "Epoch 29/50 Batch 100/937                   D_loss: 1.5436 G_loss: 0.4898\n",
      "Epoch 29/50 Batch 200/937                   D_loss: 1.6564 G_loss: 0.4230\n",
      "Epoch 29/50 Batch 300/937                   D_loss: 1.6422 G_loss: 0.5201\n",
      "Epoch 29/50 Batch 400/937                   D_loss: 1.6707 G_loss: 0.4759\n",
      "Epoch 29/50 Batch 500/937                   D_loss: 1.6611 G_loss: 0.4693\n",
      "Epoch 29/50 Batch 600/937                   D_loss: 1.6192 G_loss: 0.4117\n",
      "Epoch 29/50 Batch 700/937                   D_loss: 1.6944 G_loss: 0.4576\n",
      "Epoch 29/50 Batch 800/937                   D_loss: 1.6440 G_loss: 0.4991\n",
      "Epoch 29/50 Batch 900/937                   D_loss: 1.6369 G_loss: 0.4491\n",
      "Epoch 30/50 Batch 0/937                   D_loss: 1.5440 G_loss: 0.3504\n",
      "Epoch 30/50 Batch 100/937                   D_loss: 1.5374 G_loss: 0.3332\n",
      "Epoch 30/50 Batch 200/937                   D_loss: 1.6159 G_loss: 0.3858\n",
      "Epoch 30/50 Batch 300/937                   D_loss: 1.6522 G_loss: 0.5006\n",
      "Epoch 30/50 Batch 400/937                   D_loss: 1.6359 G_loss: 0.5025\n",
      "Epoch 30/50 Batch 500/937                   D_loss: 1.6105 G_loss: 0.5201\n",
      "Epoch 30/50 Batch 600/937                   D_loss: 1.8341 G_loss: 0.2860\n",
      "Epoch 30/50 Batch 700/937                   D_loss: 1.6649 G_loss: 0.5469\n",
      "Epoch 30/50 Batch 800/937                   D_loss: 1.4897 G_loss: 0.4739\n",
      "Epoch 30/50 Batch 900/937                   D_loss: 1.5640 G_loss: 0.3393\n",
      "Epoch 31/50 Batch 0/937                   D_loss: 1.6263 G_loss: 0.4421\n",
      "Epoch 31/50 Batch 100/937                   D_loss: 1.6452 G_loss: 0.4441\n",
      "Epoch 31/50 Batch 200/937                   D_loss: 1.6007 G_loss: 0.4201\n",
      "Epoch 31/50 Batch 300/937                   D_loss: 1.6297 G_loss: 0.4288\n",
      "Epoch 31/50 Batch 400/937                   D_loss: 1.7209 G_loss: 0.5702\n",
      "Epoch 31/50 Batch 500/937                   D_loss: 1.6530 G_loss: 0.4698\n",
      "Epoch 31/50 Batch 600/937                   D_loss: 1.7806 G_loss: 0.5048\n",
      "Epoch 31/50 Batch 700/937                   D_loss: 1.5905 G_loss: 0.3874\n",
      "Epoch 31/50 Batch 800/937                   D_loss: 1.8875 G_loss: 0.3987\n",
      "Epoch 31/50 Batch 900/937                   D_loss: 1.5886 G_loss: 0.3988\n",
      "Epoch 32/50 Batch 0/937                   D_loss: 1.6300 G_loss: 0.4245\n",
      "Epoch 32/50 Batch 100/937                   D_loss: 1.6343 G_loss: 0.2995\n",
      "Epoch 32/50 Batch 200/937                   D_loss: 1.6679 G_loss: 0.5290\n",
      "Epoch 32/50 Batch 300/937                   D_loss: 1.6727 G_loss: 0.4075\n",
      "Epoch 32/50 Batch 400/937                   D_loss: 1.6358 G_loss: 0.4155\n",
      "Epoch 32/50 Batch 500/937                   D_loss: 1.5812 G_loss: 0.4989\n",
      "Epoch 32/50 Batch 600/937                   D_loss: 1.6295 G_loss: 0.6711\n",
      "Epoch 32/50 Batch 700/937                   D_loss: 1.7685 G_loss: 0.4936\n",
      "Epoch 32/50 Batch 800/937                   D_loss: 1.5559 G_loss: 0.4236\n",
      "Epoch 32/50 Batch 900/937                   D_loss: 1.7072 G_loss: 0.4007\n",
      "Epoch 33/50 Batch 0/937                   D_loss: 1.6252 G_loss: 0.4426\n",
      "Epoch 33/50 Batch 100/937                   D_loss: 1.5501 G_loss: 0.3602\n",
      "Epoch 33/50 Batch 200/937                   D_loss: 1.5645 G_loss: 0.4761\n",
      "Epoch 33/50 Batch 300/937                   D_loss: 1.6458 G_loss: 0.4479\n",
      "Epoch 33/50 Batch 400/937                   D_loss: 1.6176 G_loss: 0.4894\n",
      "Epoch 33/50 Batch 500/937                   D_loss: 1.6444 G_loss: 0.4021\n",
      "Epoch 33/50 Batch 600/937                   D_loss: 1.8056 G_loss: 0.4156\n",
      "Epoch 33/50 Batch 700/937                   D_loss: 1.7671 G_loss: 0.4378\n",
      "Epoch 33/50 Batch 800/937                   D_loss: 1.4265 G_loss: 0.3956\n",
      "Epoch 33/50 Batch 900/937                   D_loss: 1.6865 G_loss: 0.4658\n",
      "Epoch 34/50 Batch 0/937                   D_loss: 1.7432 G_loss: 0.3891\n",
      "Epoch 34/50 Batch 100/937                   D_loss: 1.6774 G_loss: 0.4894\n",
      "Epoch 34/50 Batch 200/937                   D_loss: 1.7570 G_loss: 0.4494\n",
      "Epoch 34/50 Batch 300/937                   D_loss: 1.5700 G_loss: 0.3103\n",
      "Epoch 34/50 Batch 400/937                   D_loss: 1.6157 G_loss: 0.6218\n",
      "Epoch 34/50 Batch 500/937                   D_loss: 1.7229 G_loss: 0.3532\n",
      "Epoch 34/50 Batch 600/937                   D_loss: 1.8879 G_loss: 0.4657\n",
      "Epoch 34/50 Batch 700/937                   D_loss: 1.7831 G_loss: 0.3443\n",
      "Epoch 34/50 Batch 800/937                   D_loss: 1.6445 G_loss: 0.3609\n",
      "Epoch 34/50 Batch 900/937                   D_loss: 1.5320 G_loss: 0.4069\n",
      "Epoch 35/50 Batch 0/937                   D_loss: 1.6559 G_loss: 0.4331\n",
      "Epoch 35/50 Batch 100/937                   D_loss: 1.6798 G_loss: 0.5777\n",
      "Epoch 35/50 Batch 200/937                   D_loss: 1.7697 G_loss: 0.4526\n",
      "Epoch 35/50 Batch 300/937                   D_loss: 1.5251 G_loss: 0.3591\n",
      "Epoch 35/50 Batch 400/937                   D_loss: 1.6527 G_loss: 0.4723\n",
      "Epoch 35/50 Batch 500/937                   D_loss: 1.5557 G_loss: 0.4262\n",
      "Epoch 35/50 Batch 600/937                   D_loss: 1.4630 G_loss: 0.5888\n",
      "Epoch 35/50 Batch 700/937                   D_loss: 1.6326 G_loss: 0.3428\n",
      "Epoch 35/50 Batch 800/937                   D_loss: 1.4206 G_loss: 0.5461\n",
      "Epoch 35/50 Batch 900/937                   D_loss: 1.6918 G_loss: 0.4417\n",
      "Epoch 36/50 Batch 0/937                   D_loss: 1.5974 G_loss: 0.3903\n",
      "Epoch 36/50 Batch 100/937                   D_loss: 1.5793 G_loss: 0.5549\n",
      "Epoch 36/50 Batch 200/937                   D_loss: 1.6474 G_loss: 0.5738\n",
      "Epoch 36/50 Batch 300/937                   D_loss: 1.6623 G_loss: 0.3000\n",
      "Epoch 36/50 Batch 400/937                   D_loss: 1.7235 G_loss: 0.4217\n",
      "Epoch 36/50 Batch 500/937                   D_loss: 1.5099 G_loss: 0.3375\n",
      "Epoch 36/50 Batch 600/937                   D_loss: 1.7670 G_loss: 0.5620\n",
      "Epoch 36/50 Batch 700/937                   D_loss: 1.6724 G_loss: 0.4565\n",
      "Epoch 36/50 Batch 800/937                   D_loss: 1.5523 G_loss: 0.3390\n",
      "Epoch 36/50 Batch 900/937                   D_loss: 1.6947 G_loss: 0.2996\n",
      "Epoch 37/50 Batch 0/937                   D_loss: 1.5466 G_loss: 0.5453\n",
      "Epoch 37/50 Batch 100/937                   D_loss: 1.6457 G_loss: 0.4600\n",
      "Epoch 37/50 Batch 200/937                   D_loss: 1.6701 G_loss: 0.4308\n",
      "Epoch 37/50 Batch 300/937                   D_loss: 1.6732 G_loss: 0.5215\n",
      "Epoch 37/50 Batch 400/937                   D_loss: 1.4651 G_loss: 0.3732\n",
      "Epoch 37/50 Batch 500/937                   D_loss: 1.6234 G_loss: 0.2424\n",
      "Epoch 37/50 Batch 600/937                   D_loss: 1.6259 G_loss: 0.3624\n",
      "Epoch 37/50 Batch 700/937                   D_loss: 2.2836 G_loss: 1.9580\n",
      "Epoch 37/50 Batch 800/937                   D_loss: 1.6566 G_loss: 0.4924\n",
      "Epoch 37/50 Batch 900/937                   D_loss: 1.6983 G_loss: 0.3995\n",
      "Epoch 38/50 Batch 0/937                   D_loss: 1.5515 G_loss: 0.5300\n",
      "Epoch 38/50 Batch 100/937                   D_loss: 1.5891 G_loss: 0.5249\n",
      "Epoch 38/50 Batch 200/937                   D_loss: 1.6319 G_loss: 0.4492\n",
      "Epoch 38/50 Batch 300/937                   D_loss: 1.6470 G_loss: 0.4937\n",
      "Epoch 38/50 Batch 400/937                   D_loss: 1.6614 G_loss: 0.4144\n",
      "Epoch 38/50 Batch 500/937                   D_loss: 1.6505 G_loss: 0.4135\n",
      "Epoch 38/50 Batch 600/937                   D_loss: 1.4452 G_loss: 0.3773\n",
      "Epoch 38/50 Batch 700/937                   D_loss: 1.7463 G_loss: 0.4163\n",
      "Epoch 38/50 Batch 800/937                   D_loss: 1.6809 G_loss: 0.4229\n",
      "Epoch 38/50 Batch 900/937                   D_loss: 1.6055 G_loss: 0.5181\n",
      "Epoch 39/50 Batch 0/937                   D_loss: 1.5559 G_loss: 0.4219\n",
      "Epoch 39/50 Batch 100/937                   D_loss: 1.5366 G_loss: 0.3498\n",
      "Epoch 39/50 Batch 200/937                   D_loss: 1.6560 G_loss: 0.4789\n",
      "Epoch 39/50 Batch 300/937                   D_loss: 1.6991 G_loss: 0.4365\n",
      "Epoch 39/50 Batch 400/937                   D_loss: 1.6886 G_loss: 0.4829\n",
      "Epoch 39/50 Batch 500/937                   D_loss: 1.5755 G_loss: 0.4679\n",
      "Epoch 39/50 Batch 600/937                   D_loss: 1.7146 G_loss: 0.4477\n",
      "Epoch 39/50 Batch 700/937                   D_loss: 1.5929 G_loss: 0.3490\n",
      "Epoch 39/50 Batch 800/937                   D_loss: 1.4786 G_loss: 0.4209\n",
      "Epoch 39/50 Batch 900/937                   D_loss: 1.7585 G_loss: 0.5775\n",
      "Epoch 40/50 Batch 0/937                   D_loss: 1.5162 G_loss: 0.5118\n",
      "Epoch 40/50 Batch 100/937                   D_loss: 1.6020 G_loss: 0.2825\n",
      "Epoch 40/50 Batch 200/937                   D_loss: 1.5455 G_loss: 0.4526\n",
      "Epoch 40/50 Batch 300/937                   D_loss: 1.6753 G_loss: 0.3446\n",
      "Epoch 40/50 Batch 400/937                   D_loss: 1.4350 G_loss: 0.4689\n",
      "Epoch 40/50 Batch 500/937                   D_loss: 1.4428 G_loss: 0.4141\n",
      "Epoch 40/50 Batch 600/937                   D_loss: 1.6942 G_loss: 0.3015\n",
      "Epoch 40/50 Batch 700/937                   D_loss: 1.6704 G_loss: 0.3612\n",
      "Epoch 40/50 Batch 800/937                   D_loss: 1.6888 G_loss: 0.4145\n",
      "Epoch 40/50 Batch 900/937                   D_loss: 1.7170 G_loss: 0.4149\n",
      "Epoch 41/50 Batch 0/937                   D_loss: 1.7892 G_loss: 0.2998\n",
      "Epoch 41/50 Batch 100/937                   D_loss: 1.6195 G_loss: 0.4640\n",
      "Epoch 41/50 Batch 200/937                   D_loss: 1.7595 G_loss: 0.2939\n",
      "Epoch 41/50 Batch 300/937                   D_loss: 1.7735 G_loss: 0.3412\n",
      "Epoch 41/50 Batch 400/937                   D_loss: 1.6541 G_loss: 0.4266\n",
      "Epoch 41/50 Batch 500/937                   D_loss: 1.6563 G_loss: 0.5416\n",
      "Epoch 41/50 Batch 600/937                   D_loss: 1.4918 G_loss: 0.6615\n",
      "Epoch 41/50 Batch 700/937                   D_loss: 1.6544 G_loss: 0.5524\n",
      "Epoch 41/50 Batch 800/937                   D_loss: 1.7271 G_loss: 0.5745\n",
      "Epoch 41/50 Batch 900/937                   D_loss: 1.5491 G_loss: 0.3929\n",
      "Epoch 42/50 Batch 0/937                   D_loss: 1.9029 G_loss: 0.4877\n",
      "Epoch 42/50 Batch 100/937                   D_loss: 1.5135 G_loss: 0.4260\n",
      "Epoch 42/50 Batch 200/937                   D_loss: 1.8566 G_loss: 0.4713\n",
      "Epoch 42/50 Batch 300/937                   D_loss: 1.6928 G_loss: 0.6996\n",
      "Epoch 42/50 Batch 400/937                   D_loss: 1.5898 G_loss: 0.3552\n",
      "Epoch 42/50 Batch 500/937                   D_loss: 1.7919 G_loss: 0.4179\n",
      "Epoch 42/50 Batch 600/937                   D_loss: 1.6731 G_loss: 0.4097\n",
      "Epoch 42/50 Batch 700/937                   D_loss: 1.7490 G_loss: 0.4677\n",
      "Epoch 42/50 Batch 800/937                   D_loss: 1.6872 G_loss: 0.5879\n",
      "Epoch 42/50 Batch 900/937                   D_loss: 1.5808 G_loss: 0.4278\n",
      "Epoch 43/50 Batch 0/937                   D_loss: 1.5884 G_loss: 0.4802\n",
      "Epoch 43/50 Batch 100/937                   D_loss: 1.4321 G_loss: 0.6062\n",
      "Epoch 43/50 Batch 200/937                   D_loss: 1.4965 G_loss: 0.4440\n",
      "Epoch 43/50 Batch 300/937                   D_loss: 1.8924 G_loss: 0.3869\n",
      "Epoch 43/50 Batch 400/937                   D_loss: 1.5759 G_loss: 0.3403\n",
      "Epoch 43/50 Batch 500/937                   D_loss: 1.6910 G_loss: 0.4533\n",
      "Epoch 43/50 Batch 600/937                   D_loss: 1.4758 G_loss: 0.3400\n",
      "Epoch 43/50 Batch 700/937                   D_loss: 1.4702 G_loss: 0.4137\n",
      "Epoch 43/50 Batch 800/937                   D_loss: 1.4958 G_loss: 0.3079\n",
      "Epoch 43/50 Batch 900/937                   D_loss: 1.4680 G_loss: 0.3936\n",
      "Epoch 44/50 Batch 0/937                   D_loss: 1.4901 G_loss: 0.4648\n",
      "Epoch 44/50 Batch 100/937                   D_loss: 1.7158 G_loss: 0.3388\n",
      "Epoch 44/50 Batch 200/937                   D_loss: 1.7816 G_loss: 0.3150\n",
      "Epoch 44/50 Batch 300/937                   D_loss: 1.4946 G_loss: 0.6053\n",
      "Epoch 44/50 Batch 400/937                   D_loss: 1.4697 G_loss: 0.5486\n",
      "Epoch 44/50 Batch 500/937                   D_loss: 1.4610 G_loss: 0.4094\n",
      "Epoch 44/50 Batch 600/937                   D_loss: 1.5404 G_loss: 0.3811\n",
      "Epoch 44/50 Batch 700/937                   D_loss: 1.8750 G_loss: 0.3146\n",
      "Epoch 44/50 Batch 800/937                   D_loss: 1.6365 G_loss: 0.4209\n",
      "Epoch 44/50 Batch 900/937                   D_loss: 1.4847 G_loss: 0.4704\n",
      "Epoch 45/50 Batch 0/937                   D_loss: 1.7595 G_loss: 0.5084\n",
      "Epoch 45/50 Batch 100/937                   D_loss: 1.8268 G_loss: 0.3258\n",
      "Epoch 45/50 Batch 200/937                   D_loss: 1.6258 G_loss: 0.4710\n",
      "Epoch 45/50 Batch 300/937                   D_loss: 1.4530 G_loss: 0.5127\n",
      "Epoch 45/50 Batch 400/937                   D_loss: 1.6256 G_loss: 0.4498\n",
      "Epoch 45/50 Batch 500/937                   D_loss: 1.6970 G_loss: 0.4669\n",
      "Epoch 45/50 Batch 600/937                   D_loss: 1.5365 G_loss: 0.5405\n",
      "Epoch 45/50 Batch 700/937                   D_loss: 1.6487 G_loss: 0.3832\n",
      "Epoch 45/50 Batch 800/937                   D_loss: 1.7850 G_loss: 0.4943\n",
      "Epoch 45/50 Batch 900/937                   D_loss: 1.6111 G_loss: 0.3825\n",
      "Epoch 46/50 Batch 0/937                   D_loss: 1.6336 G_loss: 0.5029\n",
      "Epoch 46/50 Batch 100/937                   D_loss: 1.6823 G_loss: 0.5157\n",
      "Epoch 46/50 Batch 200/937                   D_loss: 1.7917 G_loss: 0.6152\n",
      "Epoch 46/50 Batch 300/937                   D_loss: 1.5688 G_loss: 0.4402\n",
      "Epoch 46/50 Batch 400/937                   D_loss: 1.5234 G_loss: 0.6426\n",
      "Epoch 46/50 Batch 500/937                   D_loss: 1.9496 G_loss: 0.1814\n",
      "Epoch 46/50 Batch 600/937                   D_loss: 1.5074 G_loss: 0.5212\n",
      "Epoch 46/50 Batch 700/937                   D_loss: 1.5597 G_loss: 0.4043\n",
      "Epoch 46/50 Batch 800/937                   D_loss: 1.7592 G_loss: 0.4712\n",
      "Epoch 46/50 Batch 900/937                   D_loss: 1.7932 G_loss: 0.3774\n",
      "Epoch 47/50 Batch 0/937                   D_loss: 1.8545 G_loss: 0.4623\n",
      "Epoch 47/50 Batch 100/937                   D_loss: 1.5883 G_loss: 0.3221\n",
      "Epoch 47/50 Batch 200/937                   D_loss: 1.5904 G_loss: 0.4160\n",
      "Epoch 47/50 Batch 300/937                   D_loss: 1.6188 G_loss: 0.3509\n",
      "Epoch 47/50 Batch 400/937                   D_loss: 1.6456 G_loss: 0.5107\n",
      "Epoch 47/50 Batch 500/937                   D_loss: 1.5962 G_loss: 0.4691\n",
      "Epoch 47/50 Batch 600/937                   D_loss: 1.5431 G_loss: 0.3643\n",
      "Epoch 47/50 Batch 700/937                   D_loss: 1.7082 G_loss: 0.5074\n",
      "Epoch 47/50 Batch 800/937                   D_loss: 1.6061 G_loss: 0.5205\n",
      "Epoch 47/50 Batch 900/937                   D_loss: 1.6342 G_loss: 0.3003\n",
      "Epoch 48/50 Batch 0/937                   D_loss: 1.6613 G_loss: 0.4664\n",
      "Epoch 48/50 Batch 100/937                   D_loss: 1.5638 G_loss: 0.6903\n",
      "Epoch 48/50 Batch 200/937                   D_loss: 1.5239 G_loss: 0.3732\n",
      "Epoch 48/50 Batch 300/937                   D_loss: 1.6739 G_loss: 0.4369\n",
      "Epoch 48/50 Batch 400/937                   D_loss: 1.5017 G_loss: 0.4757\n",
      "Epoch 48/50 Batch 500/937                   D_loss: 1.6928 G_loss: 0.2414\n",
      "Epoch 48/50 Batch 600/937                   D_loss: 1.6669 G_loss: 0.5733\n",
      "Epoch 48/50 Batch 700/937                   D_loss: 1.6688 G_loss: 0.4961\n",
      "Epoch 48/50 Batch 800/937                   D_loss: 1.7708 G_loss: 0.4410\n",
      "Epoch 48/50 Batch 900/937                   D_loss: 1.6858 G_loss: 0.2938\n",
      "Epoch 49/50 Batch 0/937                   D_loss: 1.5698 G_loss: 0.4701\n",
      "Epoch 49/50 Batch 100/937                   D_loss: 1.6698 G_loss: 0.5419\n",
      "Epoch 49/50 Batch 200/937                   D_loss: 1.4927 G_loss: 0.4256\n",
      "Epoch 49/50 Batch 300/937                   D_loss: 1.5746 G_loss: 0.3327\n",
      "Epoch 49/50 Batch 400/937                   D_loss: 1.5711 G_loss: 0.4580\n",
      "Epoch 49/50 Batch 500/937                   D_loss: 1.7288 G_loss: 0.2872\n",
      "Epoch 49/50 Batch 600/937                   D_loss: 1.7213 G_loss: 0.2976\n",
      "Epoch 49/50 Batch 700/937                   D_loss: 1.7608 G_loss: 0.4547\n",
      "Epoch 49/50 Batch 800/937                   D_loss: 1.4931 G_loss: 0.4425\n",
      "Epoch 49/50 Batch 900/937                   D_loss: 1.8178 G_loss: 0.4844\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, real_labels ) in enumerate(trainloader):\n",
    "        batch_size = imgs.shape[0]\n",
    "        imgs = imgs.to(device)\n",
    "        real_labels = real_labels.to(device)\n",
    "        real_labels = to_categorical(real_labels, n_classes).to(device)\n",
    "        #train discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        #real\n",
    "        code_input = torch.randn(batch_size, code_dim).to(device)\n",
    "        real_validity, real_label, real_code = discriminator(imgs)\n",
    "        d_adv_real_loss = adversarial_loss(real_validity, torch.ones_like(real_validity))\n",
    "        d_aux_real_loss = auxiliary_loss(real_label, torch.argmax(real_labels, 1))\n",
    "        d_con_real_loss = continuous_loss(real_code, code_input)\n",
    "        d_real_loss = d_adv_real_loss + d_aux_real_loss + d_con_real_loss\n",
    "        #fake\n",
    "        noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_labels = torch.randint(0, 10, (batch_size,)).to(device)\n",
    "        fake_labels = to_categorical(fake_labels, n_classes).to(device)\n",
    "        fake_imgs = generator(noise, fake_labels, code_input)\n",
    "        fake_validity, fake_label, fake_code = discriminator(fake_imgs.detach())\n",
    "        d_adv_fake_loss = adversarial_loss(fake_validity, torch.zeros_like(fake_validity))\n",
    "        d_aux_fake_loss = auxiliary_loss(fake_label, torch.argmax(fake_labels, 1))\n",
    "        d_con_fake_loss = continuous_loss(fake_code, code_input)\n",
    "        d_fake_loss = d_adv_fake_loss + d_aux_fake_loss + d_con_fake_loss\n",
    "        #total\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        #train generator\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_validity, fake_label, fake_code = discriminator(fake_imgs)\n",
    "        g_adv_loss = adversarial_loss(fake_validity, torch.ones_like(fake_validity))\n",
    "        g_aux_loss = auxiliary_loss(fake_label, torch.argmax(fake_labels, 1))\n",
    "        g_con_loss = continuous_loss(fake_code, code_input)\n",
    "        g_loss = g_adv_loss + g_aux_loss + g_con_loss\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        #print\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{n_epochs} Batch {i}/{len(trainloader)} \\\n",
    "                  D_loss: {d_loss.item():.4f} G_loss: {g_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate images\n",
    "noise = torch.randn(64, latent_dim).to(device)\n",
    "labels = torch.randint(0, 2, (64,)).to(device)\n",
    "labels = to_categorical(labels, n_classes).to(device)\n",
    "code = torch.randn(64, code_dim).to(device)\n",
    "gen_imgs = generator(noise, labels, code)\n",
    "gen_imgs = gen_imgs.view(gen_imgs.shape[0], 1, 28, 28)\n",
    "gen_imgs = gen_imgs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1, 28, 28)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot 3x3\n",
    "fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n",
    "cnt = 0\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axs[i,j].imshow(gen_imgs[cnt, 0], cmap='gray')\n",
    "        axs[i,j].axis('off')\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77f0ac4a1ff910c6f832be6ab53afe92115f75471ff7ffff1273b50351d0e386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
