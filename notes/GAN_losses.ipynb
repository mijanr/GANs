{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses used in training GANs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Binary Cross Entropy (BCE) Loss\n",
    "\n",
    "BCE loss is used for binary classification problems, where the output of the discriminator is a single scalar between 0 and 1. While the discriminator tries to maximize the probability of assigning the correct label to both real and fake images, the generator tries to minimize the probability of the discriminator assigning the correct label to fake images.\n",
    "\n",
    "The BCE loss is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_{\\text{BCE}}(x, y) &= -\\frac{1}{N}\\sum_{i=1}^N \\left[ y_i \\log x_i + (1-y_i) \\log (1-x_i) \\right] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $x$ is the discriminator output, $y$ is the ground truth label, and $N$ is the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9315)\n"
     ]
    }
   ],
   "source": [
    "# example for discriminator loss\n",
    "real_labels = torch.ones(10, 1)\n",
    "fake_labels = torch.zeros(10, 1)\n",
    "\n",
    "# sigmoid function \n",
    "sigmoid = nn.Sigmoid()\n",
    "# discriminator output for real data\n",
    "disc_out_real = torch.randn(10, 1)\n",
    "# discriminator output for fake data\n",
    "disc_out_fake = torch.randn(10, 1)\n",
    "\n",
    "# apply sigmoid to get probabilities\n",
    "prob_real = sigmoid(disc_out_real)\n",
    "prob_fake = sigmoid(disc_out_fake)\n",
    "\n",
    "# calculate loss\n",
    "loss_real = F.binary_cross_entropy(prob_real, real_labels)\n",
    "loss_fake = F.binary_cross_entropy(prob_fake, fake_labels)\n",
    "\n",
    "# total discriminator loss\n",
    "disc_loss = loss_real + loss_fake\n",
    "print(disc_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross Entropy (CE) Loss\n",
    "\n",
    "CE loss is used for multi-class classification problems, where the output of the discriminator is a vector of length $C$ (number of classes). Cross-entropy loss function is usually used with softmax function, and expressed as:\n",
    "\n",
    "$$L_{CE} = -\\sum_{i=1}^{N}y_i\\log(\\hat{y_i})$$\n",
    "\n",
    "where $y_i$ is the true label and $\\hat{y_i}$ is the predicted label.\n",
    "\n",
    "CE loss is used in AC-GAN, where the discriminator has two outputs: one for classifying the real/fake images, and the other for classifying the image class. The BCE loss is used for the first output, and the CE loss is used for the second output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1550)\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# target labels\n",
    "target = torch.randint(0, 10, (5,)).long()\n",
    "# predicted logits\n",
    "pred = torch.randn(5, 10)\n",
    "\n",
    "# calculate loss\n",
    "loss = ce_loss(pred, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
